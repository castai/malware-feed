import bz2
from concurrent.futures import ThreadPoolExecutor, as_completed
import configparser
import csv
import datetime
import difflib
import gzip
import hashlib
import io
import json
import logging
import lzma
import os
import re
import sys
import tarfile
import urllib.request
import zipfile
from dataclasses import dataclass, field, fields
from enum import Enum
from itertools import groupby
from pathlib import Path
from queue import Queue
from textwrap import dedent
from typing import Callable, Generator, Iterator, Self
from urllib.error import HTTPError

logging.basicConfig(
    level=logging.DEBUG,
    stream=sys.stdout,
    format="[%(asctime)s][%(levelname)s]: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S %Z",
)
logging.Formatter
logger = logging.getLogger(__name__)


class Magic(Enum):
    # TAR has no magic number
    ELF = b"\x7fELF"
    PE = b"\x4d\x5a"
    ZIP = b"\x50\x4b\x03\x04"
    ZIP_EMPTY = b"\x50\x4b\x05\x06"
    ZIP_SPANNED = b"\x50\x4b\x07\x08"
    SEVENZIP = b"\x37\x7a\xbc\xaf\x27\x1c"
    GZIP = b"\x1f\x8b"
    XZ = b"\xfd\x37\x7a\x58\x5a\x00"
    MACHO_64 = b"\xcf\xfa\xed\xfe"
    MACHO_32 = b"\xce\xfa\xed\xfe"
    MACHO_MULTI = b"\xca\xfe\xba\xbe"


@dataclass(order=True, frozen=True)
class SemVer:
    major: int = 0
    minor: int = 0
    patch: int = 0
    label: str = ""

    def __str__(self):
        res = f"{self.major}.{self.minor}.{self.patch}"
        if self.label != "":
            res += f"-{self.label}"
        return res


SEMVER_PATTERN = re.compile(
    r"(?P<major>\d+)\.?(?P<minor>\d+)*\.?(?P<patch>\d+)*(?P<label>\S*)"
)

GH_TOKEN = os.getenv("GITHUB_TOKEN", None)


def normalize_version(version: str | SemVer) -> SemVer:
    if isinstance(version, SemVer):
        return version  # already normalized
    if len(version) == 0:
        return SemVer(0, 0, 0)

    result = re.search(SEMVER_PATTERN, version)
    if result is None:
        logger.debug(f"failed to parse version '{version}'")
        return SemVer()

    *nums, label = result.groups()
    major, minor, patch = [int(n) if n is not None else 0 for n in nums]
    return SemVer(major, minor, patch, label.strip("-."))


@dataclass
class Asset:
    name: str
    dl_url: str
    created_at: str | None = None
    content_type: str | None = None


@dataclass
class Release:
    name: str
    url: str
    version: SemVer | None = None
    date: datetime.datetime | None = None
    tag: str | None = None
    tag_as_variant: bool = False
    asset_entries: list[dict] | None = None


@dataclass
class Repo:
    url: str
    category: str
    malware_name: str
    name: str | None = None
    owner: str | None = None
    is_fork: bool = False

    def __post_init__(self):
        if self.owner is None and self.name is None:
            _, self.owner, self.name = self.url.rstrip("/").rsplit("/", 2)

    def get_releases(
        self,
        crawl_limit_reached: Callable | None = None,
        force_latest_release: bool = False,
    ) -> list[dict]:
        """
        Get all releases from the repository up to a certain crawl limit.
        :param crawl_limit_reached: a function to determine if the crawl limit is reached based on the release date and/or version.
        :param force_latest_release: If True, the latest release is always fetched, regardless of other constraints.
        """
        raise NotImplementedError

    def get_forks(
        self, crawl_limit_reached: Callable | None = None
    ) -> Generator[Self, None, None]:
        raise NotImplementedError

    def get_asset_hashes_from_release(
        self, release: Release
    ) -> tuple[SemVer, list[tuple[str, str]]]:
        raise NotImplementedError


def _parse_link_header(headers: dict) -> dict[str, str] | None:
    """
    From GH docs: When a response is paginated, the response headers will include a link header.
    If the endpoint does not support pagination, or if all results fit on a single page, the link header will be omitted.

    The link header can contain these 4 links like this (but not every line must be present):
    ```link: <https://api.github.com/repositories/1300192/issues?page=2>; rel="prev",
             <https://api.github.com/repositories/1300192/issues?page=4>; rel="next",
             <https://api.github.com/repositories/1300192/issues?page=515>; rel="last",
             <https://api.github.com/repositories/1300192/issues?page=1>; rel="first"
    """
    link_header = headers.get("Link", None)
    if link_header is None:
        return None

    entries = link_header.split(",")
    pages_links = dict.fromkeys(["prev", "next", "last", "first"], None)
    for entry in entries:
        # match url and rel of link header link
        url, rel = entry.split(";")
        url = url.strip(" <>")
        rel = rel.split("=")[1].strip('"')
        pages_links[rel] = url
    return pages_links


@dataclass
class GithubReleaseInfo:
    url: str
    tag: str
    createdAt: datetime.datetime


@dataclass
class ApiRateLimit:
    limit: int
    remaining: int
    used: int
    reset_time: datetime.datetime
    resource: str


@dataclass
class GithubForkNode:
    name: str
    owner: str
    updatedAt: datetime.datetime
    url: str
    latestRelease: GithubReleaseInfo | None = None
    release_count: int = 0
    releases: dict | None = None


@dataclass
class QuotaLimitReached(Exception):
    msg: str
    resource: str  # either 'core' or 'graphql'


@dataclass
class GithubRepo(Repo):
    @staticmethod
    def parse_ratelimit_info_from_headers(headers: dict) -> ApiRateLimit:
        reset_time = datetime.datetime.fromtimestamp(
            int(headers.get("X-RateLimit-Reset", 0))
        )
        return ApiRateLimit(
            limit=int(headers.get("X-RateLimit-Limit", 0)),
            remaining=int(headers.get("X-RateLimit-Remaining", 0)),
            used=int(headers.get("X-RateLimit-Used", 0)),
            reset_time=reset_time,
            resource=headers.get("X-RateLimit-Resource", ""),
        )

    @classmethod
    def run_graphql_query(
        cls, query: str, token: str, **kwargs: dict
    ) -> tuple[dict, ApiRateLimit]:
        data = {
            "query": query,
            "variables": kwargs,
        }
        try:
            req = urllib.request.Request(
                "https://api.github.com/graphql", data=json.dumps(data).encode()
            )
            req.add_header("Authorization", "Bearer " + token)
            res = urllib.request.urlopen(req)
            rate_limit_info = cls.parse_ratelimit_info_from_headers(dict(res.headers))
            logger.debug(f"graphql: {rate_limit_info}")
            response = res.read()
            result = json.loads(response)
            return result, rate_limit_info
        except HTTPError as e:
            if e.code == 403 and "rate limit exceeded" in str(e):
                raise QuotaLimitReached("quota exceeded", "core")
            return {}, None
        except Exception as e:
            logger.error(f"Error when running GraphQL query: {e}")
            return {}, None

    def _fetch_releases(self, token: str | None) -> Generator[Release, None, None]:
        """
        Handles interaction with the Github API for loading releases.
        If a repo has more than 30 releases, the API will paginate the results.
        :yields: a single release metadata dictionary.
        """
        # initial API request does not support page parameter
        api_url = f"https://api.github.com/repos/{self.owner}/{self.name}/releases?per_page=100"
        while api_url is not None:
            try:
                request = urllib.request.Request(api_url)
                if token is not None:
                    request.add_header("Authorization", "Bearer " + token)
                response = urllib.request.urlopen(request)
                page_links = _parse_link_header(dict(response.headers))
                rate_limit_info = self.parse_ratelimit_info_from_headers(
                    dict(response.headers)
                )
                logger.debug(f"releases: {self.owner}/{self.name}:: {rate_limit_info}")
                response = response.read()
                release_metadata = json.loads(response)
                yield from map(self._process_release, release_metadata)
                if page_links is None:
                    break
                api_url = page_links["next"]
            except HTTPError as e:
                if e.code == 403 and "rate limit exceeded" in str(e):
                    raise QuotaLimitReached("quota exceeded", "core")
                else:
                    logger.error(f"Error when loading {api_url} : {e}")
                    break
            except Exception as e:
                logger.error(f"Error when loading releases for repo '{self.name}': {e}")
                break

    def get_releases(
        self,
        crawl_limit_reached: Callable | None = None,
        force_latest_release: bool = False,
    ) -> tuple[list[Release], bool]:
        """
        Get all releases from a GitHub repository up to a certain crawl limit.
        :param crawl_limit_reached: a function to determine if the crawl limit is reached based on the release date and/or version.
        :param force_latest_release: If True, the latest release is always fetched, regardless of other constraints.
        :returns: a list of releases and a flag indicating if the quota limit was reached.
        """
        force_satisified = not force_latest_release
        quota_limit_reached = False

        releases = []
        try:
            for release in self._fetch_releases(GH_TOKEN):
                if (
                    force_satisified
                    and crawl_limit_reached is not None
                    and crawl_limit_reached(release.date, release.version)
                ):
                    break

                releases.append(release)
                # force condition is definitely satisfied after providing 1 release
                force_satisified = True
        except QuotaLimitReached:
            quota_limit_reached = True
        except Exception as e:
            logger.error(f"Error when fetching releases: {e}")
        return releases, quota_limit_reached

    def _fetch_forks(self) -> Generator[GithubForkNode, None, None]:
        query = dedent("""query GetForks($owner: String!, $name: String!, $cursor: String!) {
            repository(owner:$owner, name: $name) {
                forkCount
                forks(
                    visibility: PUBLIC
                    first: 100
                    after: $cursor
                    orderBy: { field: UPDATED_AT, direction: DESC }
                ) {
                    nodes {
                        owner{login}
                        name
                        updatedAt
                        url
                        latestRelease {
                            url
                            tagName
                            createdAt
                        }
                        releases (first: 1) {
                            totalCount
                        }
                    }
                    pageInfo {
                        endCursor
                        hasNextPage
                    }
                }
            }
        }""")
        has_next_page = True
        cursor = ""
        while has_next_page:
            result, rate_limit_info = self.run_graphql_query(
                query, GH_TOKEN, owner=self.owner, name=self.name, cursor=cursor
            )
            if "errors" in result:
                logger.error(f"Error when fetching forks: {result['errors']}")
                return

            forks = result.get("data", {}).get("repository", {}).get("forks", {})
            yield from map(self._process_fork_node, forks.get("nodes", []))

            # GH pagination: endCursor can be used as the `after` arg in the forks query
            page_info = forks["pageInfo"]  # for pagination
            cursor = page_info["endCursor"]
            has_next_page = page_info["hasNextPage"]

    def get_forks(
        self, crawl_limit_reached: Callable | None = None
    ) -> Generator[Self | None, None, None]:
        """
        Get all forks of a GitHub repository ordered by their last update in descending ordere.
        :param crawl_limit_reached: a function to determine if the crawl limit is reached on the last update.
        :yields: a GithubRepo object for each fork or None if the quota limit was reached.
        """
        try:
            for fork in self._fetch_forks():
                if crawl_limit_reached is not None and crawl_limit_reached(
                    fork.updatedAt
                ):
                    break

                if fork.latestRelease is not None:
                    malware_name = self.malware_name
                    # use only the originally tracked repo not the full chain as malware name
                    if self.is_fork and " (" in malware_name:
                        malware_name = malware_name[: malware_name.index(" (")]
                    malware_name += f" ({fork.owner}/{fork.name})"

                    yield GithubRepo(
                        url=fork.url,
                        category=self.category,
                        malware_name=malware_name,
                        name=fork.name,
                        owner=fork.owner,
                    )
        except QuotaLimitReached:
            yield None  # indicate quota limit was reached

    def get_asset_hashes_from_release(
        self, release: Release
    ) -> tuple[SemVer, list[tuple[str, str]]]:
        variant_pfx = release.tag if release.tag_as_variant else None

        assets = map(self._process_asset, release.asset_entries)
        variant_hashes = calculate_asset_hashes(
            assets,
            self.name,
            self.malware_name,
            release,
            variant_prefix=variant_pfx,
        )
        return release.version, list(variant_hashes)

    @staticmethod
    def _process_asset(asset: dict) -> Asset:
        return Asset(
            name=asset.get("name", ""),
            dl_url=asset["browser_download_url"],
            created_at=asset.get("created_at", ""),
            content_type=asset.get("content_type", ""),
        )

    @staticmethod
    def _process_release(release: dict) -> Release:
        tag_name = release.get("tag_name", "")
        version = normalize_version(tag_name)
        tag_as_variant = False  # flag to explicitely use tag in the feeds variant field
        date = datetime.datetime.fromisoformat(release.get("published_at", ""))
        # if no proper version can be inferred, use the release date as version
        if version == SemVer():
            version = SemVer(
                date.year,
                date.month,
                date.day,
                label=f"{date.hour:02}:{date.minute:02}",
            )
            tag_as_variant = True

        return Release(
            name=release.get("name", ""),
            url=release.get("url", ""),
            version=version,
            date=date,
            tag=tag_name,
            asset_entries=release.get("assets", []),
            tag_as_variant=tag_as_variant,
        )

    @staticmethod
    def _process_fork_node(node: dict) -> GithubForkNode:
        latest_release = node.get("latestRelease", None)
        if latest_release is not None:
            latest_release = GithubReleaseInfo(
                url=latest_release["url"],
                tag=latest_release["tagName"],
                createdAt=datetime.datetime.fromisoformat(latest_release["createdAt"]),
            )
        return GithubForkNode(
            name=node["name"],
            owner=node["owner"]["login"],
            updatedAt=datetime.datetime.fromisoformat(node["updatedAt"]),
            url=node["url"],
            latestRelease=latest_release,
            release_count=node["releases"]["totalCount"],
            releases=node["releases"],
        )


@dataclass
class Entry:
    name: str
    category: str
    version: SemVer
    variant: str = ""
    sha256: str = ""

    def __post_init__(self):
        if not isinstance(self.version, SemVer):
            self.version = normalize_version(self.version)


@dataclass()
class Malware:
    name: str
    category: str
    hashes: dict[str, tuple[SemVer, str]] = field(default_factory=dict)
    versions: list[SemVer] = field(default_factory=list)
    repo: str = ""

    def __post_init__(self):
        # ensure versions are in descending order (latest first)
        self.versions = sorted(
            (version for version, _ in self.hashes.values()), reverse=True
        )

    def get_latest_version(self) -> SemVer | None:
        if len(self.versions) == 0:
            return None
        return self.versions[0]

    def to_entries(self) -> Generator[Entry, None, None]:
        for hash, (version, variant) in self.hashes.items():
            yield Entry(self.name, self.category, version, variant, hash)


def load_tracked_repos(src_file: str | Path) -> list[Repo]:
    """
    Read a given config file with a list of source repositories split into sections by their category.
    If no name is specified (i.e., just URL and no `<name>=`) then the repo name is used as the software name.
    ```...
       [<Category>]
       <name>=<URL>
       <URL>   # derive name from repository
    ```
    """
    repos = []
    cfg = configparser.ConfigParser(delimiters="=", allow_no_value=True)
    cfg.optionxform = str  # preserve case of key
    try:
        parsed_files = cfg.read(src_file)
        if len(parsed_files) == 0:
            raise FileNotFoundError(f"File '{src_file}' not found")

        for category in cfg.sections():
            for name, url in cfg.items(category):
                if url == "":  # if just URL was specified (i.e. no `name=`)
                    url = name  # then the URL is in the name field
                    _, name = url.rsplit("/", 1)

                RepoClass = Repo
                if "github.com" in url:
                    RepoClass = GithubRepo

                repos.append(RepoClass(url, category, name))
    except Exception as e:
        logger.error(e)
    return repos


def load_feed(feed_file: str) -> list[Entry]:
    """
    Read a feed file with a list of software releases.
    The feed file is a CSV file with the following columns:
    `name,category,version,variant,sha256`
    """
    entries = []
    try:
        with open(feed_file, "r") as f:
            reader = csv.reader(f)
            next(reader, None)  # skip header
            for line in reader:
                if len(line) > 0:
                    entries.append(Entry(*line))
    except Exception as e:
        logger.error(e)
    return entries


def save_feed(feed_file: str, malwares: list[Malware]):
    """
    Write the feed as CSV with a row per malware version, variant and its hash
    :param feed_file: The destination file to write the feed to.
    :param malwares: The list of malware entries to write to the feed.
    """
    field_names = [f.name for f in fields(Entry)]
    with open(feed_file, "w") as f:
        writer = csv.DictWriter(
            f, fieldnames=field_names, quotechar='"', quoting=csv.QUOTE_NONNUMERIC
        )
        writer.writeheader()

        for malware in malwares:
            for entry in malware.to_entries():
                if not is_asset_relevant(entry.variant):
                    continue
                row = {f.name: str(getattr(entry, f.name)) for f in fields(Entry)}
                writer.writerow(row)


def group_entries_by_name(entries: list[Entry]) -> dict[str, Malware]:
    def _merge_entries_to_malware(entries: list[Entry]) -> Malware:
        hashes = {}
        name, cat = None, None
        for e in entries:
            if name is None:
                name, cat = e.name, e.category
            hashes[e.sha256] = (e.version, e.variant)
        return Malware(name=name, category=cat, hashes=hashes)

    groups = {
        name: _merge_entries_to_malware(entries_iter)
        for name, entries_iter in groupby(entries, key=lambda x: x.name)
    }

    return groups


def diff_month(d1, d2):
    return (d1.year - d2.year) * 12 + d1.month - d2.month


def calculate_asset_hashes(
    assets: Iterator[Asset],
    name: str,
    malware_name: str,
    release: Release,
    variant_prefix: str | None = None,
) -> Generator[str, None, None]:
    """
    Process a list of assets by 1) downloading it and 2) decompressing it if necessary
    If the asset is an archive the the contents will be extracted and the primary file within will be used as the asset.
    Finally, the hash of the file will be computed.
    After processing, the variant will be determined based on its asset and release name.
    :yields: A tuple of the asset variant and its hash.
    """
    skip_content_types = [
        "application/pgp-signature",
        "application/x-x509-ca-cert",
    ]

    with ThreadPoolExecutor() as executor:
        futures = []
        for asset in assets:
            # early filter for irrelevant assets
            if asset.content_type in skip_content_types:
                continue

            asset_variant = extract_asset_variant(name, release, asset)
            # filter assets based for different OSs based based just on the variant to avoid false positives
            if not is_asset_relevant(asset_variant):  # check for OS
                continue

            future = executor.submit(
                get_uncompressed_asset_name_and_hash,
                malware_name,
                asset.name,
                asset.dl_url,
                asset_variant,
            )
            futures.append(future)

        # yield the results fot he has processing as they become available
        for future in as_completed(futures):
            if future.exception() is not None:
                logger.error(f"Error getting asset hash: {future.exception()}")
                continue

            file_name, file_hash, asset_variant = future.result()
            if asset_variant == "":
                asset_variant = file_name

            if variant_prefix is not None:
                asset_variant = f"{variant_prefix}-{asset_variant}"

            if file_hash is not None:
                yield (asset_variant, file_hash)


def extract_asset_variant(repo_name: str, release: Release, asset: Asset) -> str:
    """
    Heuristic for deriving the asset variant.
    """
    if asset.name is None:
        logger.info("😞 can't extract variant from asset because name is empty")
        return ""

    variant = asset.name

    # repo name is most likely same across assets in release, to it is not part of the variant
    variant = variant.replace(repo_name, "")

    if release.tag is not None:
        tag_name = release.tag
        if tag_name.startswith("v"):
            variant = variant.replace(tag_name, "")
            tag_name = tag_name[1:]
        variant = variant.replace(tag_name, "")
        # if tag_name in asset_name:
        #     variant = asset_name[asset_name.index(tag_name) + len(tag_name) :]

    # drop all known file extensions
    for ext in [".tar", ".zip", ".gz", ".bz2", ".xz"]:
        variant = variant.replace(ext, "")

    return variant.strip("-.")


def _download_asset(url: str) -> bytes | None:
    try:
        data = urllib.request.urlopen(url).read()
    except Exception as e:
        logger.error(f"Error when downloading asset from '{url}': {e}")
        return None
    return data


def get_uncompressed_asset_name_and_hash(
    malware_name: str,
    name: str,
    download_url: str,
    variant: str | None = None,
    calc_archive_hash: bool = False,
) -> tuple[str, str | None, str | None]:
    """
    Download the asset with the given `browser_download_url` and compute the hash of its primary binary.
    Only binary files are taken into consideration for the hash computation.
    If the asset contains multiple binaries, the one with the closest name to the malware name is used.
    :param malware_name: The name of the malware to match the binary against.
    :param name: The name of the asset.
    :param download_url: The URL to download the asset from.
    :param variant: The variant name of the asset.
    :param calc_archive_hash: Whether to calculate the hash of the entire archive.
    :return: The name, hash and passed variant_name of the primary binary file.
        If the hash is None, it means no suitable binary file was found.
    """
    data = _download_asset(download_url)
    if data is None:
        return "", None, None

    if calc_archive_hash:
        m = hashlib.sha256()
        m.update(data)
        archive_sha256 = m.hexdigest()
        logger.info(f"Archive: {name}: {archive_sha256}")

    filters = [  # any positive matches will be REMOVED from the final result
        is_text_file,
        is_shared_library,
    ]

    is_archive, processor_fn = determine_asset_processor(name, data)
    if processor_fn is None:
        logger.debug(f"No processor found for '{name}'. Skipping...")
        return "", None, None

    file_hashes = {}
    for file_name, data in processor_fn(name, data):
        if len(data) == 0:
            logger.debug(f"Skipping empty file '{file_name}'")
            continue
        try:
            # apply filter only to archive files; decision logic for individual files is done by processor
            if (
                is_archive
                and any((filter_fn(file_name, data) for filter_fn in filters))
                or is_unsupported_os_binary(file_name, data)
            ):
                continue  # skip any filtered files

            # drop the path information, if it's part of the name
            if "/" in file_name:
                _, file_name = file_name.rsplit("/", 1)

            m = hashlib.sha256()
            m.update(data)
            file_hashes[file_name] = m.hexdigest()
        except Exception as e:
            logger.error(f"Error calculating has of '{file_name}': {e}")

    # in case of multiple options, the closest match will be selected as the hashed asset
    num_candidates = len(file_hashes)
    if num_candidates == 1:
        file_name, file_hash = list(file_hashes.items())[0]
        return file_name, file_hash, variant
    elif num_candidates == 0:
        if is_archive:  # an archive should have a suitable candidate
            logger.info(f"no matching file found for {malware_name}/{name}")
        return None, None, None
    elif num_candidates > 1:
        closest_matches = difflib.get_close_matches(
            malware_name.lower(), file_hashes.keys(), n=1, cutoff=0.0
        )
        if len(closest_matches) > 0:
            closest_match = closest_matches[0]
            logger.debug(
                f"Got {len(file_hashes)} hashes for {name}, use closest: '{closest_match}'"
            )
            return closest_match, file_hashes[closest_match], variant
        logger.info(f"no matching file found for {name}")
        return None, None, None


type FilesGenerator = Generator[tuple[str, bytes], None, None]


def determine_asset_processor(
    name: str, data: bytes
) -> tuple[bool, Callable[[str, bytes], FilesGenerator]]:
    """
    Determine the processor function for the asset based on its name.
    :returns: A tuple of whether the asset is a archive (has multiple files) and the processor function.
    """
    p = Path(name)
    # tarball support decompression natively, no need for recursive processing
    # if tarball is compressed, usually an extra suffix is added (e.g. .tar.gz)
    if ".tar" in p.suffixes[-2:] or p.suffix == ".tgz":
        return True, process_tarball

    def use_file_as_is(name, data):
        return [(name, data)]

    match p.suffix:
        case ".zip":
            return True, process_zip_archive
        case ".bz2":
            return False, process_bzip2_file
        case ".gz":
            return False, process_gzip_file
        case ".xz":
            return False, process_xz_file
        case ".sh" | ".js" | ".ts" | ".py":
            return False, use_file_as_is

    if is_binary(data):
        return False, use_file_as_is
    return False, None


def process_tarball(name: str, data: bytes) -> FilesGenerator:
    compression_sfx = name.split(".")[-1]
    mode = f"r:{compression_sfx}"
    try:
        with tarfile.open(fileobj=io.BytesIO(data), mode=mode) as f:
            for file_info in f:
                # process only normal file; skip directories and other files
                if file_info.type == tarfile.REGTYPE:
                    content = f.extractfile(file_info.name).read()
                    yield file_info.name, content
    except tarfile.CompressionError as e:
        logger.error(f"Error when decompressing '{name}'", e)
        return name, None


def process_zip_archive(name: str, data: bytes) -> FilesGenerator:
    with zipfile.ZipFile(io.BytesIO(data)) as f:
        for file_info in f.filelist:
            if file_info.is_dir():
                continue

            content = f.read(file_info)
            yield file_info.filename, content


def process_bzip2_file(name: str, data: bytes) -> FilesGenerator:
    yield name.replace(".bz2", ""), bz2.decompress(data)


def process_gzip_file(name: str, data: bytes) -> FilesGenerator:
    yield name.replace(".gz", ""), gzip.decompress(data)


def process_xz_file(name: str, data: bytes) -> FilesGenerator:
    yield name.replace(".xz", ""), lzma.decompress(data)


def is_asset_relevant(variant_name: str) -> bool:
    """
    Estimate the target OS, and kind of asset of check if is relevant for the feed.
    :param variant_name: The name of the asset variant to check.
    :returns: True if the asset is for a relevant OS, False otherwise.
    """
    irrelevant_extensions = [".exe", ".bat", ".txt"]
    if any(variant_name.endswith(ext) for ext in irrelevant_extensions):
        return False

    OS_KEYWORDS = [
        "darwin",
        "macos",
        "mac64",
        "win32",
        "win64",
        "windows",
        "android",
        "freebsd",
        "netbsd",
        "openbsd",
        "dragonfly",
        "plan9",
        "solaris",
    ]
    META_FILES = ["sha256sums", "checksum"]
    variant_name = variant_name.lower()
    return not any(kw in variant_name for kw in OS_KEYWORDS + META_FILES)


def is_text_file(name: str, content: bytes) -> bool:
    is_binary = any(content.startswith(magic.value) for magic in Magic)
    if is_binary:
        return False
    try:
        # only text files can be decoded with UTF-8, binaries will raise an exception
        content.decode("utf-8")
        return True
    except UnicodeDecodeError:
        return False
    except Exception as e:
        logger.error(f"Other exception when testing file type of '{name}':", e)
    return False


def is_binary(content: bytes) -> bool:
    MAGIC_BYTES = [
        Magic.ELF,
    ]
    return any(content.startswith(magic.value) for magic in MAGIC_BYTES)


def is_unsupported_os_binary(name: str, content: bytes) -> bool:
    MAGIC_BYTES = [
        Magic.MACHO_64,
        Magic.MACHO_32,
        Magic.MACHO_MULTI,
        Magic.PE,
    ]
    return any(content.startswith(magic.value) for magic in MAGIC_BYTES)


def is_compressed_file(name: str, content: bytes) -> bool:
    MAGIC_BYTES = [
        b"PK",  # ZIP file
        b"BZ",
    ]
    return any(content.startswith(magic) for magic in MAGIC_BYTES)


def is_shared_library(name: str, *args) -> bool:
    return name.endswith(".so") or name.endswith(".dll") or name.endswith(".sys")


def update_malware_entries(
    repo: Repo,
    malware: Malware,
    crawl_limit_reached: Callable | None = None,
    last_n_months: int = 6,
    force_latest_release: bool = False,
) -> tuple[int, bool]:
    """
    Update the entries of a malware in the feed with the latest releases from a repository.
    If the malware has previous versions, new releases are all up to the last known version.
    If no previous version is in the feed, then the repository is queried for releases up to `last_n_months` ago.
    :param repo: The repository to fetch the releases from.
    :param malware: the malware to update.
    :param crawl_limit_reached: a function to determine if the crawl limit is reached based on the release date and/or version.
    :param last_n_months: The number of months to go back in time to fetch releases.
    :param force_latest_release: If True, the latest release is always fetched, regardless of other constraints.
    :returns: The number of updates made to the malware and a flag indicating if the quota limit was reached.
    """

    # the category of the malware is defined in the config file.
    # Repos are loaded from this config file, so they contain the correct name of the category.
    if malware.category != repo.category:
        malware.category = repo.category

    releases, limit_reached = repo.get_releases(
        crawl_limit_reached,
        force_latest_release,
        # malware.get_latest_version(), last_n_months, force_latest_release
    )
    releases = filter_unlisted_releases(malware, releases)

    total_num_assets = _count_all_assets(releases)
    updates = 0

    if total_num_assets == 0:
        logger.info(
            f"No new releases found for '{malware.name}' in the last {last_n_months} months."
        )
        return 0, False
    else:
        logger.debug(
            f"Updating {malware.name}: got {len(releases)} releases ({total_num_assets} assets)"
        )

    for release in releases:
        logger.debug(f"processing '{malware.name}' {release.tag or "(no tag name)"}")
        version, variant_hashes = repo.get_asset_hashes_from_release(release)
        if version not in malware.versions:
            malware.versions.append(version)

        for variant, file_hash in variant_hashes:
            # resolve any duplicates by using latest/longest option
            if (prior_entry := malware.hashes.get(file_hash, None)) is not None:
                prior_version, prior_variant = prior_entry
                # always use latest version (as it's the most recent information)
                version = max(version, prior_version)
                variant = max(variant, prior_variant)  # use most explicit variant
            malware.hashes[file_hash] = version, variant
            updates += 1
    return updates, limit_reached


def filter_unlisted_releases(
    malware: Malware, releases: list[Release]
) -> list[Release]:
    filtered_releases = []
    for release in releases:
        if release.version not in malware.versions:
            filtered_releases.append(release)

    diff = len(releases) - len(filtered_releases)
    if diff > 0:
        logger.debug(f"Ignoring {diff} releases because these are already listed")
    return filtered_releases


def _count_all_assets(releases: list[Release]) -> int:
    total = 0
    for release in releases:
        total += len(release.asset_entries)
    return total


def deduplicate_entries(malwares: list[Malware]) -> tuple[list[Malware], int]:
    """
    Deduplicate the hashes across all malware entries.
    Deduplication is done very naivly by keeping the first entry of the binary (without regarding provided information/source repo)
    It's possible, that malicious assets are mirrored in other repositories (e.g. to avoid detection).
    If more entries have the same hash, it means they are the same binary, so merge the respective information and keep only one version.
    :param malwares: The list of malware entries to deduplicate.
    :return: The deduplicated list of malware entries and the number of hashes that were dropped.
    """
    seen_hashes = set()
    total_dropped = 0
    for m in malwares:
        keep_hashes = {}
        drop_hashes = {}
        for h, v in m.hashes.items():
            if h in seen_hashes:
                drop_hashes[h] = v
            else:
                keep_hashes[h] = v
            seen_hashes.add(h)
        m.hashes = keep_hashes
        if (num_dropped := len(drop_hashes)) > 0:
            total_dropped += num_dropped
            logger.info(f"Dropped {num_dropped} duplicate hashes for '{m.name}'")

    return malwares, total_dropped


def create_crawl_limit_check(
    last_n_months: int,
    min_version: SemVer | None,
) -> Callable[[datetime.datetime, str | None], bool]:
    """
    Create a function to check if the given release is within the crawl scope.
    If the version is not specified, get all releases of the past `last_n_months` months.
    :param last_n_months: The number of months to go back in time to fetch releases.
    :param min_version: Get all releases up to (i.e. excluding) this version.
        If no min_version is specified, the last_n_months parameter is used.
    :return: A function to check if the given release is within the crawl scope.
    """
    today = datetime.date.today()

    def _check_crawl_limit(
        date: datetime.datetime, version: str | SemVer | None = None
    ) -> bool:
        if version is not None and (
            min_version is not None and min_version > SemVer(0, 0, 0)
        ):
            # stop processing releases if we reach the min. version bound
            if version <= min_version:
                return True
        else:
            if diff_month(today, date) > last_n_months:
                return True

        return False

    return _check_crawl_limit


def save_remaining_repos(repos: list[Repo], dst_path: Path):
    """
    Save the remaining repositories in the queue to a CSV file for later retrieval.
    :param repos: The list of remaining repositories to save.
    :dst_path: The destination file to save the queue to.
    """
    field_names = [f.name for f in fields(Repo)]
    with open(dst_path, "w") as f:
        writer = csv.DictWriter(
            f, fieldnames=field_names, quotechar='"', quoting=csv.QUOTE_NONNUMERIC
        )
        writer.writeheader()
        for repo in repos:
            row = {f.name: str(getattr(repo, f.name)) for f in fields(Repo)}
            writer.writerow(row)


def load_stashed_repos(src_path: Path) -> list[Repo]:
    """
    Load the stashed repositories from a CSV file into the queue, if such a file exists.
    :param src_path: The source file to repositories from.
    :returns: The list of stashed repositories to process.
    """
    if not src_path.exists():
        return

    repos = []
    try:
        with open(src_path, "r") as f:
            reader = csv.reader(f)
            next(reader, None)  # skip header
            for line in reader:
                if len(line) > 0:
                    repos.append(GithubRepo(*line))
    except Exception as e:
        logger.error(e)
    return repos


def process_repository_queue(
    queue: Queue,
    feed_entries: dict[str, Malware],
    crawl_forks: bool = False,
    last_n_months: int = 12,
) -> tuple[int, bool]:
    """
    Process the repositorise in the queue by checking for new releases and updating the malware entries in the feed.
    If forks are crawled as well, then forked repositories with releases will be appended to the queue.
    :param queue: The queue of repositories to process.
    :param feed_entries: The dictionary of malware entries to update.
    :param crawl_forks: Whether to crawl the forks of the repositories.
    :param last_n_months: The number of months to go back in time to fetch releases.
    :returns: The total number of updates made to the feed and a flag indicating if the quota limit was reached.
    """
    total_num_updates = 0
    quota_limit_reached = False

    while not queue.empty() and not quota_limit_reached:
        repo = queue.get()

        malware = feed_entries.get(repo.malware_name, None)
        is_new_malware = malware is None
        if is_new_malware:
            logger.info(f"New Malware '{repo.malware_name}' added to feed")
            feed_entries[repo.malware_name] = malware = Malware(
                repo.malware_name, repo.category, {}
            )

        check_fn = create_crawl_limit_check(last_n_months, malware.get_latest_version())
        num_updates, quota_limit_reached = update_malware_entries(
            repo,
            malware,
            last_n_months=last_n_months,
            crawl_limit_reached=check_fn,
            force_latest_release=is_new_malware,
        )
        total_num_updates += num_updates

        if crawl_forks:
            for fork in repo.get_forks(check_fn):
                # None is canary value to indicate quota limit was reached
                if fork is None:
                    # put this repo back into the queue to retry later
                    queue.put(repo)
                    quota_limit_reached = True
                logger.debug(f"Add fork '{fork.malware_name}' to queue")
                queue.put(fork)

    return total_num_updates, quota_limit_reached


def main(repos_source_file: Path, feed_dir: Path, last_n_months: int = 12):
    feed_file = feed_dir / "latest.csv"
    queue_stash_file = feed_dir / "queue.csv"

    repos_to_check = []
    if queue_stash_file.exists():
        repos_to_check = load_stashed_repos(queue_stash_file)
        queue_stash_file.unlink(missing_ok=True)
    else:  # continue previous run, no need to start over
        tracked_repos = load_tracked_repos(repos_source_file)
        repos_to_check = tracked_repos

    known_entries = load_feed(feed_file)
    # group entries by their name and get latest version
    feed_entries = group_entries_by_name(known_entries)

    proc_queue = Queue()
    list(map(proc_queue.put, repos_to_check))
    total_num_updates, quota_limit_reached = process_repository_queue(
        proc_queue,
        feed_entries,
        crawl_forks=GH_TOKEN is not None,
        last_n_months=last_n_months,
    )
    if quota_limit_reached:
        save_remaining_repos(list(proc_queue.queue), queue_stash_file)

    if total_num_updates > 0:
        feed_entries, num_dropped = deduplicate_entries(list(feed_entries.values()))

        # only save feed if there any actual updates
        if total_num_updates - num_dropped > 0:
            # override latest feed and provide a copy of today as backup
            save_feed(feed_file, feed_entries)
            # today = datetime.date.today().strftime("%Y-%m-%d")
            # shutil.copy(feed_file, feed_dir / f"{today}.csv")


if __name__ == "__main__":
    main(
        repos_source_file=Path("./config.ini"),
        feed_dir=Path("./data"),
        last_n_months=6,
    )
