import configparser
import csv
import datetime
import difflib
from pathlib import Path
import shutil
import tarfile
import io
import hashlib
import json
import zipfile
from collections import defaultdict, namedtuple
from dataclasses import asdict, dataclass, field, fields
from itertools import groupby
from typing import Generator
from urllib import request

# Naive assumption: all releases use semantic versioning
SemVer = namedtuple(
    "SemVer", ["major", "minor", "patch", "label"], defaults=[0, 0, 0, ""]
)


def _semver_to_str(semver: SemVer) -> str:
    res = f"{semver.major}.{semver.minor}.{semver.patch}"
    if semver.label != "":
        res += f"-{semver.label}"
    return res


SemVer.__str__ = _semver_to_str


def normalize_version(version: str) -> SemVer:
    if isinstance(version, SemVer):
        return version  # already normalized
    if len(version) == 0:
        return SemVer(0, 0, 0)
    if version.startswith("v"):
        version = version[1:]
    label = ""
    if "-" in version:
        version, label = version.split("-", maxsplit=1)
    return SemVer(*map(int, [v for v in version.split(".") if v]), label)


@dataclass
class Repo:
    url: str
    category: str
    malware_name: str
    name: str | None = None
    owner: str | None = None

    def __post_init__(self):
        _, self.owner, self.name = self.url.rsplit("/", 2)

    def get_releases(
        self, min_version: SemVer | None = None, since: int = 6
    ) -> list[dict]:
        raise NotImplementedError

    def get_asset_hashes_from_release(
        self, release: dict
    ) -> tuple[SemVer, list[tuple[str, str]]]:
        raise NotImplementedError


@dataclass
class GithubRepo(Repo):
    def get_releases(
        self, min_version: SemVer | None = None, last_n_months: int = 6
    ) -> list[dict]:
        """
        Get all releases from a GitHub repository up to a certain version.
        If the version is not specified, get all releases of the past `last_n_months` months.
        """
        api_url = f"https://api.github.com/repos/{self.owner}/{self.name}/releases"
        try:
            response = request.urlopen(api_url).read()
            release_metadata = json.loads(response)
        except Exception as e:
            print(f"Error when loading releases for repo '{self.name}': {e}")
        today = datetime.date.today()

        releases = []
        for release in release_metadata:
            if (
                min_version is not None and min_version > SemVer(0, 0, 0)
            ):  # we have a lower_bound for the version, use this instead of a time-frame
                version = normalize_version(release.get("tag_name", ""))
                if (
                    version <= min_version
                ):  # stop processing releases if we reach the min. version bound
                    break
            else:
                release_date = release.get("published_at", "")
                release_date = datetime.datetime.fromisoformat(release_date)
                if diff_month(today, release_date) > last_n_months:
                    break
            releases.append(release)
        return releases

    def get_asset_hashes_from_release(
        self, release: dict
    ) -> tuple[SemVer, list[tuple[str, str]]]:
        version = normalize_version(release.get("tag_name", None))

        release_name = release.get("name", None)
        if release_name is None or release_name == "'":
            print("No release name provided in release")

        variant_hashes = []
        skip_content_types = [
            "application/pgp-signature",
            "application/x-x509-ca-cert",
        ]
        for asset in release.get("assets", []):
            content_type = asset.get("content_type", "")
            if content_type in skip_content_types:
                continue

            asset_name = asset.get("name", "")
            if asset_name == "SHA256SUMS":
                pass
                # data = request.urlopen(asset["browser_download_url"]).read()
                # print(data.decode("utf-8"))
            elif asset_name == "SHA256SUMS.sig":
                pass
            else:
                asset_variant = extract_asset_variant(release, asset)
                file_name, file_hash = get_uncompressed_asset_name_and_hash(
                    malware_name=self.malware_name, **asset
                )

                # if asset_variant == "":
                #     asset_variant = file_name

                if file_hash is not None:
                    variant_hashes.append((asset_variant, file_hash))
        return version, variant_hashes


@dataclass
class Entry:
    name: str
    category: str
    version: SemVer
    variant: str = ""
    sha256: str = ""

    def __post_init__(self):
        if not isinstance(self.version, SemVer):
            self.version = normalize_version(self.version)


@dataclass()
class Malware:
    name: str
    category: str
    versions: dict[SemVer, list[tuple[str, str]]] = field(
        default_factory=lambda: defaultdict(list)
    )
    hashes: dict[str, tuple[SemVer, str]] = field(default_factory=dict)
    versions: list[SemVer] = field(default_factory=list)
    repo: str = ""

    def __post_init__(self):
        # ensure versions are in descending order (latest first)
        self.versions = sorted(
            (version for version, _ in self.hashes.values()), reverse=True
        )

    def get_latest_version(self) -> SemVer | None:
        if len(self.versions) == 0:
            return None
        return list(self.versions)[0]

    def to_entries(self) -> Generator[Entry, None, None]:
        for hash, (version, variant) in self.hashes.items():
            yield Entry(self.name, self.category, version, variant, hash)


def load_tracked_repos(src_file: str) -> list[Repo]:
    """
    Read a given config file with a list of source repositories split into sections by their category.
    If no name is specified (i.e., just URL and no `<name>=`) then the repo name is used as the software name.
    ```...
       [<Category>]
       <name>=<URL>
       <URL>   # derive name from repository
    ```
    """
    repos = []
    cfg = configparser.ConfigParser(delimiters="=", allow_no_value=True)
    cfg.optionxform = str  # preserve case of key
    try:
        parsed_files = cfg.read(src_file)
        if len(parsed_files) == 0:
            raise FileNotFoundError(f"File '{src_file}' not found")

        for category in cfg.sections():
            for name, url in cfg.items(category):
                if url == "":  # if just URL was specified (i.e. no `name=`)
                    url = name  # then the URL is in the name field
                    _, name = url.rsplit("/", 1)

                RepoClass = Repo
                if "github.com" in url:
                    RepoClass = GithubRepo

                repos.append(RepoClass(url, category, name))
    except Exception as e:
        print(e)
    return repos


def load_feed(feed_file: str) -> list[Entry]:
    """
    Read a feed file with a list of software releases.
    The feed file is a CSV file with the following columns:
    `name,category,version,variant,sha256`
    """
    entries = []
    try:
        with open(feed_file, "r") as f:
            reader = csv.reader(f)
            headers = next(reader, None)  # skip header
            for line in reader:
                entries.append(Entry(*line))
    except Exception as e:
        print(e)
    return entries


def save_feed(feed_file: str, malwares: dict[str, Malware]):
    """
    Write the feed as CSV with a row per malware version, variant and its hash
    :param feed_file: The destination file to write the feed to.
    :param malwares: The dictionary of malware entries to write to the feed.
    """
    field_names = [f.name for f in fields(Entry)]
    with open(feed_file, "w") as f:
        writer = csv.DictWriter(
            f, fieldnames=field_names, quotechar='"', quoting=csv.QUOTE_NONNUMERIC
        )
        writer.writeheader()

        for malware in malwares.values():
            for entry in malware.to_entries():
                writer.writerow({k: str(v) for k, v in asdict(entry).items()})


def group_entries_by_name(entries: list[Entry]) -> dict[str, Malware]:
    def _merge_entries_to_malware(entries: list[Entry]) -> Malware:
        hashes = {}
        name, cat = None, None
        for e in entries:
            if name is None:
                name, cat = e.name, e.category
            hashes[e.sha256] = (e.version, e.variant)
        return Malware(name=name, category=cat, hashes=hashes)

    groups = {
        name: _merge_entries_to_malware(entries_iter)
        for name, entries_iter in groupby(entries, key=lambda x: x.name)
    }

    return groups


def diff_month(d1, d2):
    return (d1.year - d2.year) * 12 + d1.month - d2.month


def extract_asset_variant(release: dict, asset: dict) -> str:
    """
    Heuristic for deriving the asset variant.
    """
    asset_name = asset.get("name", None)
    if asset_name is None:
        print("ðŸ˜ž can't extract variant from asset because name is empty")
        return ""

    variant = asset_name
    tag_name = release.get("tag_name", None)
    if tag_name is not None:
        if tag_name.startswith("v"):
            tag_name = tag_name[1:]
        if tag_name in asset_name:
            variant = asset_name[asset_name.index(tag_name) + len(tag_name) :]

    # drop all known file extensions
    for ext in [".tar", ".zip", ".gz", ".bz2", ".xz"]:
        variant = variant.replace(ext, "")

    return variant.strip("-.")


def get_uncompressed_asset_name_and_hash(
    malware_name: str,
    name: str,
    browser_download_url: str,
    calc_archive_hash: bool = False,
    **kwargs,
) -> tuple[str, str | None]:
    """
    Download the asset with the given `browser_download_url` and compute the hash of its primary binary.
    Only binary files are taken into consideration for the hash computation.
    If the asset contains multiple binaries, the one with the closest name to the malware name is used.
    :param malware_name: The name of the malware to match the binary against.
    :param name: The name of the asset.
    :param browser_download_url: The URL to download the asset from.
    :param calc_archive_hash: Whether to calculate the hash of the entire archive.
    :return: The name and hash of the primary binary file.
        If the hash is None, it means no suitable binary file was found.
    """
    try:
        data = request.urlopen(browser_download_url).read()
    except Exception as e:
        print(
            f"Error when downloading asset '{name}' from '{browser_download_url}': {e}"
        )

    if calc_archive_hash:
        m = hashlib.sha256()
        m.update(data)
        archive_sha256 = m.hexdigest()
        print(f"Archive: {name}: {archive_sha256}")

    filters = [  # any positive matches will be REMOVED from the final result
        is_text_file,
        is_shared_library,
    ]

    processor_nf = None
    if ".tar" in name:
        processor_nf = process_tarball
    elif name.endswith(".zip"):
        processor_nf = process_zip_archive

    if processor_nf is None:
        print(f"No processor found for '{name}'. Skipping...")
        return "", None

    file_hashes = {}
    for file_name, data in processor_nf(name, data):
        try:
            if any((filter_fn(file_name, data) for filter_fn in filters)):
                continue  # skip any filtered files

            # drop the path information, if it's part of the name
            if "/" in file_name:
                _, file_name = file_name.rsplit("/", 1)

            m = hashlib.sha256()
            m.update(data)
            file_hashes[file_name] = m.hexdigest()
        except Exception as e:
            print(f"Error calculating has of '{file_name}': {e}")

    num_candidates = len(file_hashes)
    if num_candidates == 1:
        return list(file_hashes.items())[0]
    elif num_candidates == 0:
        print(f"no matching file found for {name}")
        return None, None
    elif num_candidates > 1:
        [closest_match] = difflib.get_close_matches(
            malware_name.lower(), file_hashes.keys(), n=1, cutoff=0.3
        )
        print(
            f"Got {len(file_hashes)} hashes for {name}, use closest: '{closest_match}'"
        )
        return closest_match, file_hashes[closest_match]


def process_tarball(name: str, data: bytes) -> Generator[tuple[str, bytes], None, None]:
    compression_sfx = name.split(".")[-1]
    mode = f"r:{compression_sfx}"
    with tarfile.open(fileobj=io.BytesIO(data), mode=mode) as f:
        for file_info in f:
            # process only normal file; skip directories and other files
            if file_info.type == tarfile.REGTYPE:
                content = f.extractfile(file_info.name).read()
                yield file_info.name, content
    return None


def process_zip_archive(
    name: str, data: bytes
) -> Generator[tuple[str, bytes], None, None]:
    with zipfile.ZipFile(io.BytesIO(data)) as f:
        for file_info in f.filelist:
            if file_info.is_dir():
                continue

            content = f.read(file_info)
            yield file_info.filename, content


def is_text_file(name: str, content: bytes) -> bool:
    try:
        # only text files can be decoded with UTF-8, binaries will raise an exception
        content.decode("utf-8")
        return True
    except UnicodeDecodeError:
        return False
    except Exception as e:
        print(f"Other exception when testing file type of '{name}':", e)
    return False


def is_shared_library(name: str, *args) -> bool:
    return name.endswith(".so") or name.endswith(".dll") or name.endswith(".sys")


def update_malware_entries(repo: Repo, malware: Malware, last_n_months: int = 6):
    """
    Update the entries of a malware in the feed with the latest releases from a repository.
    If the malware has previous versions, new releases are all up to the last known version.
    If no previous version is in the feed, then the repository is queried for releases up to `last_n_months` ago.
    :param repo: The repository to fetch the releases from.
    :param malware: the malware to update.
    :param last_n_months: The number of months to go back in time to fetch releases.
    """

    # the category of the malware is defined in the config file.
    # Repos are loaded from this config file, so they contain the correct name of the category.
    if malware.category != repo.category:
        malware.category = repo.category

    releases = repo.get_releases(malware.get_latest_version(), last_n_months)
    total_num_assets = _count_all_assets(releases)

    if total_num_assets == 0:
        print(
            f"No new releases found for '{malware.name}' in the last {last_n_months} months."
        )
    else:
        print(
            f"updating {malware.name}: got {len(releases)} releases (total {total_num_assets} assets)"
        )

    for release in releases:
        # print(
        #     f"processing '{malware.name}' v.{release.get("tag_name", "(no tag name)")}"
        # )
        version, variant_hashes = repo.get_asset_hashes_from_release(release)
        if version not in malware.versions:
            malware.versions[version] = []

        for variant, file_hash in variant_hashes:
            # resolve any duplicates by using latest/longest option
            if (prior_entry := malware.hashes.get(file_hash, None)) is not None:
                prior_version, prior_variant = prior_entry
                # always use latest version (as it's the most recent information)
                version = max(version, prior_version)
                variant = max(variant, prior_variant)  # use most explicit variant
            malware.hashes[file_hash] = version, variant



def _count_all_assets(repos: list[dict]) -> int:
    total = 0
    for repo in repos:
        total += len(repo.get("assets", []))
    return total


def main():
    REPOS_SOURCE_FILE = "./data/tracked_repos.ini"
    FEED_DIR = Path("./data")
    FEED_FILE = FEED_DIR / "latest.csv"
    LAST_N_MONTHS = 12

    repos = load_tracked_repos(REPOS_SOURCE_FILE)
    known_entries = load_feed(FEED_FILE)

    # group entries by their name and get lates version
    malwares = group_entries_by_name(known_entries)

    # if there is a Malware in the feed which is no longer tracked, it will be kept in the feed but not updated
    for repo in repos:
        malware = malwares.get(repo.malware_name, None)
        if malware is None:
            print(f"New Malware '{repo.malware_name}' added to feed")
            malwares[repo.malware_name] = malware = Malware(
                repo.malware_name, repo.category, {}
            )
        update_malware_entries(repo, malware, last_n_months=LAST_N_MONTHS)

    # override latest feed and provide a copy of today as backup
    save_feed(FEED_FILE, malwares)
    today = datetime.date.today().strftime("%Y-%m-%d")
    shutil.copy(FEED_FILE, FEED_DIR / f"{today}.csv")


if __name__ == "__main__":
    main()
