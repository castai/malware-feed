import configparser
import csv
import datetime
import difflib
import hashlib
import io
import json
import logging
import re
import shutil
import sys
import tarfile
import urllib.request
import zipfile
from collections import defaultdict
from dataclasses import dataclass, field, fields
from itertools import groupby
from pathlib import Path
from typing import Generator

logging.basicConfig(
    level=logging.INFO,
    stream=sys.stdout,
    format="[%(asctime)s][%(levelname)s]: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S %Z",
)
logging.Formatter
logger = logging.getLogger(__name__)


@dataclass(order=True, frozen=True)
class SemVer:
    major: int = 0
    minor: int = 0
    patch: int = 0
    label: str = ""

    def __str__(self):
        res = f"{self.major}.{self.minor}.{self.patch}"
        if self.label != "":
            res += f"-{self.label}"
        return res


SEMVER_PATTERN = re.compile(
    r"(?P<major>\d+)\.?(?P<minor>\d+)*\.?(?P<patch>\d+)*(?P<label>\S*)"
)


def normalize_version(version: str) -> SemVer:
    if isinstance(version, SemVer):
        return version  # already normalized
    if len(version) == 0:
        return SemVer(0, 0, 0)

    result = re.search(SEMVER_PATTERN, version)
    if result is None:
        logger.error(f"failed to parse version '{version}'")
        return SemVer()

    *nums, label = result.groups()
    major, minor, patch = [int(n) if n is not None else 0 for n in nums]
    return SemVer(major, minor, patch, label.strip("-."))


@dataclass
class Repo:
    url: str
    category: str
    malware_name: str
    name: str | None = None
    owner: str | None = None

    def __post_init__(self):
        _, self.owner, self.name = self.url.rsplit("/", 2)

    def get_releases(
        self,
        min_version: SemVer | None = None,
        last_n_months: int = 6,
        force_latest_release: bool = False,
    ) -> list[dict]:
        """
        Get all releases from the repository up to a certain version.
        If the minimum version is not specified, get all releases of the past `last_n_months` months.
        :param min_version: Get all releases up to (i.e. excluding) this version.
        :param last_n_months: The number of months to go back in time to fetch releases.
        :param force_latest_release: If True, the latest release is always fetched, regardless of other constraints.
        """
        raise NotImplementedError

    def get_asset_hashes_from_release(
        self, release: dict
    ) -> tuple[SemVer, list[tuple[str, str]]]:
        raise NotImplementedError


def _parse_link_header(headers: dict) -> dict[str, str] | None:
    """
    From GH docs: When a response is paginated, the response headers will include a link header.
    If the endpoint does not support pagination, or if all results fit on a single page, the link header will be omitted.

    The link header can contain these 4 links like this (but not every line must be present):
    ```link: <https://api.github.com/repositories/1300192/issues?page=2>; rel="prev",
             <https://api.github.com/repositories/1300192/issues?page=4>; rel="next",
             <https://api.github.com/repositories/1300192/issues?page=515>; rel="last",
             <https://api.github.com/repositories/1300192/issues?page=1>; rel="first"
    """
    link_header = headers.get("Link", None)
    if link_header is None:
        return None

    entries = link_header.split(",")
    pages_links = dict.fromkeys(["prev", "next", "last", "first"], None)
    for entry in entries:
        # match url and rel of link header link
        url, rel = entry.split(";")
        url = url.strip("<>")
        rel = rel.split("=")[1].strip('"')
        pages_links[rel] = url
    return pages_links


@dataclass
class GithubRepo(Repo):
    def _fetch_releases(self) -> Generator[dict, None, None]:
        """
        Handles interaction with the Github API for loading releases.
        If a repo has more than 30 releases, the API will paginate the results.
        :yields: a single release metadata dictionary.
        """
        # initial API request does not support page parameter
        api_url = f"https://api.github.com/repos/{self.owner}/{self.name}/releases"
        while api_url is not None:
            try:
                request = urllib.request.urlopen(api_url)
                page_links = _parse_link_header(request.headers)
                response = request.read()
                release_metadata = json.loads(response)
                yield from release_metadata
                if page_links is None:
                    break
                api_url = page_links["next"]
            except Exception as e:
                logger.error(f"Error when loading releases for repo '{self.name}': {e}")
                break

    def get_releases(
        self,
        min_version: SemVer | None = None,
        last_n_months: int = 6,
        force_latest_release: bool = False,
    ) -> list[dict]:
        """
        Get all releases from a GitHub repository up to a certain version.
        If the version is not specified, get all releases of the past `last_n_months` months.
        :param min_version: Get all releases up to (i.e. excluding) this version.
        :param last_n_months: The number of months to go back in time to fetch releases.
        :param force_latest_release: If True, the latest release is always fetched, regardless of other constraints.
        """
        today = datetime.date.today()
        force_satisified = not force_latest_release

        releases = []
        for release in self._fetch_releases():
            # we have a lower_bound for the version, use this instead of a time-frame
            if min_version is not None and min_version > SemVer(0, 0, 0):
                version = normalize_version(release.get("tag_name", ""))
                # stop processing releases if we reach the min. version bound
                if version <= min_version and force_satisified:
                    break
            else:
                release_date = release.get("published_at", "")
                release_date = datetime.datetime.fromisoformat(release_date)
                if diff_month(today, release_date) > last_n_months and force_satisified:
                    break
            releases.append(release)
            # force condition is definitely satisfied after providing 1 release
            force_satisified = True
        return releases

    def get_asset_hashes_from_release(
        self, release: dict
    ) -> tuple[SemVer, list[tuple[str, str]]]:
        version = normalize_version(release.get("tag_name", None))

        release_name = release.get("name", None)
        if release_name is None or release_name == "'":
            logger.info("No release name provided in release")

        variant_hashes = []
        skip_content_types = [
            "application/pgp-signature",
            "application/x-x509-ca-cert",
        ]
        for asset in release.get("assets", []):
            content_type = asset.get("content_type", "")
            if content_type in skip_content_types:
                continue

            asset_name = asset.get("name", "")
            if asset_name == "SHA256SUMS":
                pass
                # data = request.urlopen(asset["browser_download_url"]).read()
                # logger.info(data.decode("utf-8"))
            elif asset_name == "SHA256SUMS.sig":
                pass
            else:
                asset_variant = extract_asset_variant(release, asset)
                file_name, file_hash = get_uncompressed_asset_name_and_hash(
                    malware_name=self.malware_name, **asset
                )

                # if asset_variant == "":
                #     asset_variant = file_name

                if file_hash is not None:
                    variant_hashes.append((asset_variant, file_hash))
        return version, variant_hashes


@dataclass
class Entry:
    name: str
    category: str
    version: SemVer
    variant: str = ""
    sha256: str = ""

    def __post_init__(self):
        if not isinstance(self.version, SemVer):
            self.version = normalize_version(self.version)


@dataclass()
class Malware:
    name: str
    category: str
    versions: dict[SemVer, list[tuple[str, str]]] = field(
        default_factory=lambda: defaultdict(list)
    )
    hashes: dict[str, tuple[SemVer, str]] = field(default_factory=dict)
    versions: list[SemVer] = field(default_factory=list)
    repo: str = ""

    def __post_init__(self):
        # ensure versions are in descending order (latest first)
        self.versions = sorted(
            (version for version, _ in self.hashes.values()), reverse=True
        )

    def get_latest_version(self) -> SemVer | None:
        if len(self.versions) == 0:
            return None
        return self.versions[0]

    def to_entries(self) -> Generator[Entry, None, None]:
        for hash, (version, variant) in self.hashes.items():
            yield Entry(self.name, self.category, version, variant, hash)


def load_tracked_repos(src_file: str) -> list[Repo]:
    """
    Read a given config file with a list of source repositories split into sections by their category.
    If no name is specified (i.e., just URL and no `<name>=`) then the repo name is used as the software name.
    ```...
       [<Category>]
       <name>=<URL>
       <URL>   # derive name from repository
    ```
    """
    repos = []
    cfg = configparser.ConfigParser(delimiters="=", allow_no_value=True)
    cfg.optionxform = str  # preserve case of key
    try:
        parsed_files = cfg.read(src_file)
        if len(parsed_files) == 0:
            raise FileNotFoundError(f"File '{src_file}' not found")

        for category in cfg.sections():
            for name, url in cfg.items(category):
                if url == "":  # if just URL was specified (i.e. no `name=`)
                    url = name  # then the URL is in the name field
                    _, name = url.rsplit("/", 1)

                RepoClass = Repo
                if "github.com" in url:
                    RepoClass = GithubRepo

                repos.append(RepoClass(url, category, name))
    except Exception as e:
        logger.error(e)
    return repos


def load_feed(feed_file: str) -> list[Entry]:
    """
    Read a feed file with a list of software releases.
    The feed file is a CSV file with the following columns:
    `name,category,version,variant,sha256`
    """
    entries = []
    try:
        with open(feed_file, "r") as f:
            reader = csv.reader(f)
            headers = next(reader, None)  # skip header
            for line in reader:
                entries.append(Entry(*line))
    except Exception as e:
        logger.error(e)
    return entries


def save_feed(feed_file: str, malwares: dict[str, Malware]):
    """
    Write the feed as CSV with a row per malware version, variant and its hash
    :param feed_file: The destination file to write the feed to.
    :param malwares: The dictionary of malware entries to write to the feed.
    """
    field_names = [f.name for f in fields(Entry)]
    with open(feed_file, "w") as f:
        writer = csv.DictWriter(
            f, fieldnames=field_names, quotechar='"', quoting=csv.QUOTE_NONNUMERIC
        )
        writer.writeheader()

        for malware in malwares.values():
            for entry in malware.to_entries():
                row = {f.name: str(getattr(entry, f.name)) for f in fields(Entry)}
                writer.writerow(row)


def group_entries_by_name(entries: list[Entry]) -> dict[str, Malware]:
    def _merge_entries_to_malware(entries: list[Entry]) -> Malware:
        hashes = {}
        name, cat = None, None
        for e in entries:
            if name is None:
                name, cat = e.name, e.category
            hashes[e.sha256] = (e.version, e.variant)
        return Malware(name=name, category=cat, hashes=hashes)

    groups = {
        name: _merge_entries_to_malware(entries_iter)
        for name, entries_iter in groupby(entries, key=lambda x: x.name)
    }

    return groups


def diff_month(d1, d2):
    return (d1.year - d2.year) * 12 + d1.month - d2.month


def extract_asset_variant(release: dict, asset: dict) -> str:
    """
    Heuristic for deriving the asset variant.
    """
    asset_name = asset.get("name", None)
    if asset_name is None:
        logger.info("😞 can't extract variant from asset because name is empty")
        return ""

    variant = asset_name
    tag_name = release.get("tag_name", None)
    if tag_name is not None:
        if tag_name.startswith("v"):
            tag_name = tag_name[1:]
        if tag_name in asset_name:
            variant = asset_name[asset_name.index(tag_name) + len(tag_name) :]

    # drop all known file extensions
    for ext in [".tar", ".zip", ".gz", ".bz2", ".xz"]:
        variant = variant.replace(ext, "")

    return variant.strip("-.")


def get_uncompressed_asset_name_and_hash(
    malware_name: str,
    name: str,
    browser_download_url: str,
    calc_archive_hash: bool = False,
    **kwargs,
) -> tuple[str, str | None]:
    """
    Download the asset with the given `browser_download_url` and compute the hash of its primary binary.
    Only binary files are taken into consideration for the hash computation.
    If the asset contains multiple binaries, the one with the closest name to the malware name is used.
    :param malware_name: The name of the malware to match the binary against.
    :param name: The name of the asset.
    :param browser_download_url: The URL to download the asset from.
    :param calc_archive_hash: Whether to calculate the hash of the entire archive.
    :return: The name and hash of the primary binary file.
        If the hash is None, it means no suitable binary file was found.
    """
    try:
        data = urllib.request.urlopen(browser_download_url).read()
    except Exception as e:
        logger.error(
            f"Error when downloading asset '{name}' from '{browser_download_url}': {e}"
        )
        return "", None

    if calc_archive_hash:
        m = hashlib.sha256()
        m.update(data)
        archive_sha256 = m.hexdigest()
        logger.info(f"Archive: {name}: {archive_sha256}")

    filters = [  # any positive matches will be REMOVED from the final result
        is_text_file,
        is_shared_library,
    ]

    processor_nf = None
    if ".tar" in name:
        processor_nf = process_tarball
    elif name.endswith(".zip"):
        processor_nf = process_zip_archive

    if processor_nf is None:
        logger.info(f"No processor found for '{name}'. Skipping...")
        return "", None

    file_hashes = {}
    for file_name, data in processor_nf(name, data):
        try:
            if any((filter_fn(file_name, data) for filter_fn in filters)):
                continue  # skip any filtered files

            # drop the path information, if it's part of the name
            if "/" in file_name:
                _, file_name = file_name.rsplit("/", 1)

            m = hashlib.sha256()
            m.update(data)
            file_hashes[file_name] = m.hexdigest()
        except Exception as e:
            logger.error(f"Error calculating has of '{file_name}': {e}")

    num_candidates = len(file_hashes)
    if num_candidates == 1:
        return list(file_hashes.items())[0]
    elif num_candidates == 0:
        logger.info(f"no matching file found for {name}")
        return None, None
    elif num_candidates > 1:
        [closest_match] = difflib.get_close_matches(
            malware_name.lower(), file_hashes.keys(), n=1, cutoff=0.3
        )
        logger.debug(
            f"Got {len(file_hashes)} hashes for {name}, use closest: '{closest_match}'"
        )
        return closest_match, file_hashes[closest_match]


def process_tarball(name: str, data: bytes) -> Generator[tuple[str, bytes], None, None]:
    compression_sfx = name.split(".")[-1]
    mode = f"r:{compression_sfx}"
    with tarfile.open(fileobj=io.BytesIO(data), mode=mode) as f:
        for file_info in f:
            # process only normal file; skip directories and other files
            if file_info.type == tarfile.REGTYPE:
                content = f.extractfile(file_info.name).read()
                yield file_info.name, content
    return None


def process_zip_archive(
    name: str, data: bytes
) -> Generator[tuple[str, bytes], None, None]:
    with zipfile.ZipFile(io.BytesIO(data)) as f:
        for file_info in f.filelist:
            if file_info.is_dir():
                continue

            content = f.read(file_info)
            yield file_info.filename, content


def is_text_file(name: str, content: bytes) -> bool:
    try:
        # only text files can be decoded with UTF-8, binaries will raise an exception
        content.decode("utf-8")
        return True
    except UnicodeDecodeError:
        return False
    except Exception as e:
        logger.error(f"Other exception when testing file type of '{name}':", e)
    return False


def is_shared_library(name: str, *args) -> bool:
    return name.endswith(".so") or name.endswith(".dll") or name.endswith(".sys")


def update_malware_entries(
    repo: Repo,
    malware: Malware,
    last_n_months: int = 6,
    force_latest_release: bool = False,
) -> int:
    """
    Update the entries of a malware in the feed with the latest releases from a repository.
    If the malware has previous versions, new releases are all up to the last known version.
    If no previous version is in the feed, then the repository is queried for releases up to `last_n_months` ago.
    :param repo: The repository to fetch the releases from.
    :param malware: the malware to update.
    :param last_n_months: The number of months to go back in time to fetch releases.
    :param force_latest_release: If True, the latest release is always fetched, regardless of other constraints.
    :return: The number of updates for the malware.
    """

    # the category of the malware is defined in the config file.
    # Repos are loaded from this config file, so they contain the correct name of the category.
    if malware.category != repo.category:
        malware.category = repo.category

    releases = repo.get_releases(
        malware.get_latest_version(), last_n_months, force_latest_release
    )
    releases = filter_unlisted_releases(malware, releases)

    total_num_assets = _count_all_assets(releases)
    updates = 0

    if total_num_assets == 0:
        logger.info(
            f"No new releases found for '{malware.name}' in the last {last_n_months} months."
        )
        return 0
    else:
        logger.info(
            f"Updating {malware.name}: got {len(releases)} releases ({total_num_assets} assets)"
        )

    for release in releases:
        logger.debug(
            f"processing '{malware.name}' {release.get("tag_name", "(no tag name)")}"
        )
        version, variant_hashes = repo.get_asset_hashes_from_release(release)
        if version not in malware.versions:
            malware.versions.append(version)

        for variant, file_hash in variant_hashes:
            # resolve any duplicates by using latest/longest option
            if (prior_entry := malware.hashes.get(file_hash, None)) is not None:
                prior_version, prior_variant = prior_entry
                # always use latest version (as it's the most recent information)
                version = max(version, prior_version)
                variant = max(variant, prior_variant)  # use most explicit variant
            malware.hashes[file_hash] = version, variant
            updates += 1
    return updates


def filter_unlisted_releases(malware: Malware, releases: list[dict]) -> list[dict]:
    filtered_releases = []
    for release in releases:
        version = normalize_version(release.get("tag_name", ""))
        if version not in malware.versions:
            filtered_releases.append(release)
    logger.debug(
        f"Ignoring {len(releases) - len(filtered_releases)} releases because these are already listed"
    )
    return filtered_releases


def _count_all_assets(repos: list[dict]) -> int:
    total = 0
    for repo in repos:
        total += len(repo.get("assets", []))
    return total


def main():
    REPOS_SOURCE_FILE = "./data/tracked_repos.ini"
    FEED_DIR = Path("./data")
    FEED_FILE = FEED_DIR / "latest.csv"
    LAST_N_MONTHS = 6

    repos = load_tracked_repos(REPOS_SOURCE_FILE)
    known_entries = load_feed(FEED_FILE)

    # group entries by their name and get lates version
    malwares = group_entries_by_name(known_entries)

    # if there is a Malware in the feed which is no longer tracked, it will be kept in the feed but not updated
    num_updates = 0
    for repo in repos:
        malware = malwares.get(repo.malware_name, None)
        is_new_malware = malware is None
        if is_new_malware:
            logger.debug(f"New Malware '{repo.malware_name}' added to feed")
            malwares[repo.malware_name] = malware = Malware(
                repo.malware_name, repo.category, {}
            )
        num_updates += update_malware_entries(
            repo,
            malware,
            last_n_months=LAST_N_MONTHS,
            force_latest_release=is_new_malware,
        )

    if num_updates > 0:
        # override latest feed and provide a copy of today as backup
        save_feed(FEED_FILE, malwares)
        today = datetime.date.today().strftime("%Y-%m-%d")
        shutil.copy(FEED_FILE, FEED_DIR / f"{today}.csv")
    else:
        logger.info(f"No updates found for {len(repos)} tracked repositories")


if __name__ == "__main__":
    main()
