import bz2
import configparser
import csv
import datetime
import difflib
import gzip
import hashlib
import io
import json
import logging
import lzma
import re
import shutil
import sys
import tarfile
import urllib.request
import zipfile
from collections import defaultdict
from dataclasses import dataclass, field, fields
from enum import Enum
from itertools import groupby
from pathlib import Path
from typing import Callable, Generator, Iterator

logging.basicConfig(
    level=logging.INFO,
    stream=sys.stdout,
    format="[%(asctime)s][%(levelname)s]: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S %Z",
)
logging.Formatter
logger = logging.getLogger(__name__)


class Magic(Enum):
    # TAR has no magic number
    ELF = b"\x7fELF"
    PE = b"\x4d\x5a"
    ZIP = b"\x50\x4b\x03\x04"
    ZIP_EMPTY = b"\x50\x4b\x05\x06"
    ZIP_SPANNED = b"\x50\x4b\x07\x08"
    SEVENZIP = b"\x37\x7a\xbc\xaf\x27\x1c"
    GZIP = b"\x1f\x8b"
    XZ = b"\xfd\x37\x7a\x58\x5a\x00"
    MACHO_64 = b"\xcf\xfa\xed\xfe"
    MACHO_32 = b"\xce\xfa\xed\xfe"
    MACHO_MULTI = b"\xca\xfe\xba\xbe"


@dataclass(order=True, frozen=True)
class SemVer:
    major: int = 0
    minor: int = 0
    patch: int = 0
    label: str = ""

    def __str__(self):
        res = f"{self.major}.{self.minor}.{self.patch}"
        if self.label != "":
            res += f"-{self.label}"
        return res


SEMVER_PATTERN = re.compile(
    r"(?P<major>\d+)\.?(?P<minor>\d+)*\.?(?P<patch>\d+)*(?P<label>\S*)"
)


def normalize_version(version: str) -> SemVer:
    if isinstance(version, SemVer):
        return version  # already normalized
    if len(version) == 0:
        return SemVer(0, 0, 0)

    result = re.search(SEMVER_PATTERN, version)
    if result is None:
        logger.warning(f"failed to parse version '{version}'")
        return SemVer()

    *nums, label = result.groups()
    major, minor, patch = [int(n) if n is not None else 0 for n in nums]
    return SemVer(major, minor, patch, label.strip("-."))


@dataclass
class Repo:
    url: str
    category: str
    malware_name: str
    name: str | None = None
    owner: str | None = None

    def __post_init__(self):
        _, self.owner, self.name = self.url.rsplit("/", 2)

    def get_releases(
        self,
        min_version: SemVer | None = None,
        last_n_months: int = 6,
        force_latest_release: bool = False,
    ) -> list[dict]:
        """
        Get all releases from the repository up to a certain version.
        If the minimum version is not specified, get all releases of the past `last_n_months` months.
        :param min_version: Get all releases up to (i.e. excluding) this version.
            If no min_version is specified, the last_n_months parameter is used.
        :param last_n_months: The number of months to go back in time to fetch releases.
        :param force_latest_release: If True, the latest release is always fetched, regardless of other constraints.
        """
        raise NotImplementedError

    def get_asset_hashes_from_release(
        self, release: dict
    ) -> tuple[SemVer, list[tuple[str, str]]]:
        raise NotImplementedError


def _parse_link_header(headers: dict) -> dict[str, str] | None:
    """
    From GH docs: When a response is paginated, the response headers will include a link header.
    If the endpoint does not support pagination, or if all results fit on a single page, the link header will be omitted.

    The link header can contain these 4 links like this (but not every line must be present):
    ```link: <https://api.github.com/repositories/1300192/issues?page=2>; rel="prev",
             <https://api.github.com/repositories/1300192/issues?page=4>; rel="next",
             <https://api.github.com/repositories/1300192/issues?page=515>; rel="last",
             <https://api.github.com/repositories/1300192/issues?page=1>; rel="first"
    """
    link_header = headers.get("Link", None)
    if link_header is None:
        return None

    entries = link_header.split(",")
    pages_links = dict.fromkeys(["prev", "next", "last", "first"], None)
    for entry in entries:
        # match url and rel of link header link
        url, rel = entry.split(";")
        url = url.strip(" <>")
        rel = rel.split("=")[1].strip('"')
        pages_links[rel] = url
    return pages_links


@dataclass
class Asset:
    name: str
    dl_url: str
    created_at: str | None = None
    content_type: str | None = None


@dataclass
class GithubRepo(Repo):
    def _fetch_releases(self) -> Generator[dict, None, None]:
        """
        Handles interaction with the Github API for loading releases.
        If a repo has more than 30 releases, the API will paginate the results.
        :yields: a single release metadata dictionary.
        """
        # initial API request does not support page parameter
        api_url = f"https://api.github.com/repos/{self.owner}/{self.name}/releases"
        while api_url is not None:
            try:
                request = urllib.request.urlopen(api_url)
                page_links = _parse_link_header(dict(request.headers))
                response = request.read()
                release_metadata = json.loads(response)
                yield from release_metadata
                if page_links is None:
                    break
                api_url = page_links["next"]
            except Exception as e:
                logger.error(f"Error when loading releases for repo '{self.name}': {e}")
                break

    def get_releases(
        self,
        min_version: SemVer | None = None,
        last_n_months: int = 6,
        force_latest_release: bool = False,
    ) -> list[dict]:
        """
        Get all releases from a GitHub repository up to a certain version.
        If the version is not specified, get all releases of the past `last_n_months` months.
        :param min_version: Get all releases up to (i.e. excluding) this version.
            If no min_version is specified, the last_n_months parameter is used.
        :param last_n_months: The number of months to go back in time to fetch releases.
        :param force_latest_release: If True, the latest release is always fetched, regardless of other constraints.
        """
        today = datetime.date.today()
        force_satisified = not force_latest_release

        releases = []
        for release in self._fetch_releases():
            # we have a lower_bound for the version, use this instead of a time-frame
            if min_version is not None and min_version > SemVer(0, 0, 0):
                version = normalize_version(release.get("tag_name", ""))
                # stop processing releases if we reach the min. version bound
                if version <= min_version and force_satisified:
                    break
            else:
                release_date = release.get("published_at", "")
                release_date = datetime.datetime.fromisoformat(release_date)
                if diff_month(today, release_date) > last_n_months and force_satisified:
                    break
            releases.append(release)
            # force condition is definitely satisfied after providing 1 release
            force_satisified = True
        return releases

    def get_asset_hashes_from_release(
        self, release: dict
    ) -> tuple[SemVer, list[tuple[str, str]]]:
        tag_name = release.get("tag_name", None)
        version = normalize_version(tag_name)
        variant_pfx = None
        if version == SemVer(0, 0, 0):
            # if the tag is not a proper version, use it as variant prefix instead
            # and use the date of the release as the version instead
            variant_pfx = tag_name
            published_at_str = release.get("published_at", "")
            published_at = datetime.datetime.fromisoformat(published_at_str)
            version = SemVer(
                published_at.year,
                published_at.month,
                published_at.day,
                label=f"{published_at.hour:02}:{published_at.minute:02}",
            )

        release_name = release.get("name", None)
        if release_name is None or release_name == "'":
            logger.info("No release name provided in release")

        assets = map(self._parse_asset, release.get("assets", []))
        variant_hashes = process_assets(
            assets,
            self.name,
            self.malware_name,
            release,
            variant_prefix=variant_pfx,
        )
        return version, list(variant_hashes)

    @staticmethod
    def _parse_asset(asset: dict) -> Asset:
        return Asset(
            name=asset.get("name", ""),
            dl_url=asset["browser_download_url"],
            created_at=asset.get("created_at", ""),
            content_type=asset.get("content_type", ""),
        )


@dataclass
class Entry:
    name: str
    category: str
    version: SemVer
    variant: str = ""
    sha256: str = ""

    def __post_init__(self):
        if not isinstance(self.version, SemVer):
            self.version = normalize_version(self.version)


@dataclass()
class Malware:
    name: str
    category: str
    versions: dict[SemVer, list[tuple[str, str]]] = field(
        default_factory=lambda: defaultdict(list)
    )
    hashes: dict[str, tuple[SemVer, str]] = field(default_factory=dict)
    versions: list[SemVer] = field(default_factory=list)
    repo: str = ""

    def __post_init__(self):
        # ensure versions are in descending order (latest first)
        self.versions = sorted(
            (version for version, _ in self.hashes.values()), reverse=True
        )

    def get_latest_version(self) -> SemVer | None:
        if len(self.versions) == 0:
            return None
        return self.versions[0]

    def to_entries(self) -> Generator[Entry, None, None]:
        for hash, (version, variant) in self.hashes.items():
            yield Entry(self.name, self.category, version, variant, hash)


def load_tracked_repos(src_file: str) -> list[Repo]:
    """
    Read a given config file with a list of source repositories split into sections by their category.
    If no name is specified (i.e., just URL and no `<name>=`) then the repo name is used as the software name.
    ```...
       [<Category>]
       <name>=<URL>
       <URL>   # derive name from repository
    ```
    """
    repos = []
    cfg = configparser.ConfigParser(delimiters="=", allow_no_value=True)
    cfg.optionxform = str  # preserve case of key
    try:
        parsed_files = cfg.read(src_file)
        if len(parsed_files) == 0:
            raise FileNotFoundError(f"File '{src_file}' not found")

        for category in cfg.sections():
            for name, url in cfg.items(category):
                if url == "":  # if just URL was specified (i.e. no `name=`)
                    url = name  # then the URL is in the name field
                    _, name = url.rsplit("/", 1)

                RepoClass = Repo
                if "github.com" in url:
                    RepoClass = GithubRepo

                repos.append(RepoClass(url, category, name))
    except Exception as e:
        logger.error(e)
    return repos


def load_feed(feed_file: str) -> list[Entry]:
    """
    Read a feed file with a list of software releases.
    The feed file is a CSV file with the following columns:
    `name,category,version,variant,sha256`
    """
    entries = []
    try:
        with open(feed_file, "r") as f:
            reader = csv.reader(f)
            headers = next(reader, None)  # skip header
            for line in reader:
                if len(line) > 0:
                    entries.append(Entry(*line))
    except Exception as e:
        logger.error(e)
    return entries


def save_feed(feed_file: str, malwares: list[Malware]):
    """
    Write the feed as CSV with a row per malware version, variant and its hash
    :param feed_file: The destination file to write the feed to.
    :param malwares: The list of malware entries to write to the feed.
    """
    field_names = [f.name for f in fields(Entry)]
    with open(feed_file, "w") as f:
        writer = csv.DictWriter(
            f, fieldnames=field_names, quotechar='"', quoting=csv.QUOTE_NONNUMERIC
        )
        writer.writeheader()

        for malware in malwares:
            for entry in malware.to_entries():
                if not is_asset_relevant(entry.variant):
                    continue
                row = {f.name: str(getattr(entry, f.name)) for f in fields(Entry)}
                writer.writerow(row)


def group_entries_by_name(entries: list[Entry]) -> dict[str, Malware]:
    def _merge_entries_to_malware(entries: list[Entry]) -> Malware:
        hashes = {}
        name, cat = None, None
        for e in entries:
            if name is None:
                name, cat = e.name, e.category
            hashes[e.sha256] = (e.version, e.variant)
        return Malware(name=name, category=cat, hashes=hashes)

    groups = {
        name: _merge_entries_to_malware(entries_iter)
        for name, entries_iter in groupby(entries, key=lambda x: x.name)
    }

    return groups


def diff_month(d1, d2):
    return (d1.year - d2.year) * 12 + d1.month - d2.month


def process_assets(
    assets: Iterator[Asset],
    name: str,
    malware_name: str,
    release,
    variant_prefix: str | None = None,
) -> Generator[str, None, None]:
    """
    Process a list of assets by 1) downloading it and 2) decompressing it if necessary
    If the asset is an archive the the contents will be extracted and the primary file within will be used as the asset.
    Finally, the hash of the file will be computed.
    After processing, the variant will be determined based on its asset and release name.
    :yields: A tuple of the asset variant and its hash.
    """
    skip_content_types = [
        "application/pgp-signature",
        "application/x-x509-ca-cert",
    ]

    for asset in assets:
        # early filter for irrelevant assets
        if asset.content_type in skip_content_types:
            continue

        asset_variant = extract_asset_variant(name, release, asset)
        # filter assets based for different OSs based based just on the variant to avoid false positives
        if not is_asset_relevant(asset_variant):  # check for OS
            continue

        file_name, file_hash = get_uncompressed_asset_name_and_hash(
            malware_name=malware_name, name=asset.name, download_url=asset.dl_url
        )

        if asset_variant == "":
            asset_variant = file_name

        if variant_prefix is not None:
            asset_variant = f"{variant_prefix}-{asset_variant}"

        if file_hash is not None:
            yield (asset_variant, file_hash)


def extract_asset_variant(repo_name: str, release: dict, asset: Asset) -> str:
    """
    Heuristic for deriving the asset variant.
    """
    if asset.name is None:
        logger.info("ðŸ˜ž can't extract variant from asset because name is empty")
        return ""

    variant = asset.name

    # repo name is most likely same across assets in release, to it is not part of the variant
    variant = variant.replace(repo_name, "")

    tag_name = release.get("tag_name", None)
    if tag_name is not None:
        if tag_name.startswith("v"):
            variant = variant.replace(tag_name, "")
            tag_name = tag_name[1:]
        variant = variant.replace(tag_name, "")
        # if tag_name in asset_name:
        #     variant = asset_name[asset_name.index(tag_name) + len(tag_name) :]

    # drop all known file extensions
    for ext in [".tar", ".zip", ".gz", ".bz2", ".xz"]:
        variant = variant.replace(ext, "")

    return variant.strip("-.")


def _download_asset(url: str) -> bytes | None:
    try:
        data = urllib.request.urlopen(url).read()
    except Exception as e:
        logger.error(f"Error when downloading asset from '{url}': {e}")
        return "", None
    return data


def get_uncompressed_asset_name_and_hash(
    malware_name: str,
    name: str,
    download_url: str,
    calc_archive_hash: bool = False,
) -> tuple[str, str | None]:
    """
    Download the asset with the given `browser_download_url` and compute the hash of its primary binary.
    Only binary files are taken into consideration for the hash computation.
    If the asset contains multiple binaries, the one with the closest name to the malware name is used.
    :param malware_name: The name of the malware to match the binary against.
    :param name: The name of the asset.
    :param download_url: The URL to download the asset from.
    :param calc_archive_hash: Whether to calculate the hash of the entire archive.
    :return: The name and hash of the primary binary file.
        If the hash is None, it means no suitable binary file was found.
    """
    data = _download_asset(download_url)
    if data is None:
        return "", None

    if calc_archive_hash:
        m = hashlib.sha256()
        m.update(data)
        archive_sha256 = m.hexdigest()
        logger.info(f"Archive: {name}: {archive_sha256}")

    filters = [  # any positive matches will be REMOVED from the final result
        is_text_file,
        is_shared_library,
    ]

    is_archive, processor_fn = determine_asset_processor(name, data)
    if processor_fn is None:
        logger.info(f"No processor found for '{name}'. Skipping...")
        return "", None

    file_hashes = {}
    for file_name, data in processor_fn(name, data):
        if len(data) == 0:
            logger.debug(f"Skipping empty file '{file_name}'")
            continue
        try:
            # apply filter only to archive files; decision logic for individual files is done by processor
            if (
                is_archive
                and any((filter_fn(file_name, data) for filter_fn in filters))
                or is_unsupported_os_binary(file_name, data)
            ):
                continue  # skip any filtered files

            # drop the path information, if it's part of the name
            if "/" in file_name:
                _, file_name = file_name.rsplit("/", 1)

            m = hashlib.sha256()
            m.update(data)
            file_hashes[file_name] = m.hexdigest()
        except Exception as e:
            logger.error(f"Error calculating has of '{file_name}': {e}")

    # in case of multiple options, the closest match will be selected as the hashed asset
    num_candidates = len(file_hashes)
    if num_candidates == 1:
        return list(file_hashes.items())[0]
    elif num_candidates == 0:
        if is_archive:  # an archive should have a suitable candidate
            logger.info(f"no matching file found for {name}")
        return None, None
    elif num_candidates > 1:
        closest_matches = difflib.get_close_matches(
            malware_name.lower(), file_hashes.keys(), n=1, cutoff=0.0
        )
        if len(closest_matches) > 0:
            closest_match = closest_matches[0]
            logger.debug(
                f"Got {len(file_hashes)} hashes for {name}, use closest: '{closest_match}'"
            )
            return closest_match, file_hashes[closest_match]
        logger.info(f"no matching file found for {name}")
        return None, None


type FilesGenerator = Generator[tuple[str, bytes], None, None]


def determine_asset_processor(
    name: str, data: bytes
) -> tuple[bool, Callable[[str, bytes], FilesGenerator]]:
    """
    Determine the processor function for the asset based on its name.
    :returns: A tuple of whether the asset is a archive (has multiple files) and the processor function.
    """
    p = Path(name)
    # tarball support decompression natively, no need for recursive processing
    # if tarball is compressed, usually an extra suffix is added (e.g. .tar.gz)
    if ".tar" in p.suffixes[-2:] or p.suffix == ".tgz":
        return True, process_tarball

    def use_file_as_is(name, data):
        return [(name, data)]

    match p.suffix:
        case ".zip":
            return True, process_zip_archive
        case ".bz2":
            return False, process_bzip2_file
        case ".gz":
            return False, process_gzip_file
        case ".xz":
            return False, process_xz_file
        case ".sh" | ".js" | ".ts" | ".py":
            return False, use_file_as_is

    if is_binary(data):
        return False, use_file_as_is
    return False, None


def process_tarball(name: str, data: bytes) -> FilesGenerator:
    compression_sfx = name.split(".")[-1]
    mode = f"r:{compression_sfx}"
    try:
        with tarfile.open(fileobj=io.BytesIO(data), mode=mode) as f:
            for file_info in f:
                # process only normal file; skip directories and other files
                if file_info.type == tarfile.REGTYPE:
                    content = f.extractfile(file_info.name).read()
                    yield file_info.name, content
    except tarfile.CompressionError as e:
        logger.error(f"Error when decompressing '{name}'", e)
        return name, None


def process_zip_archive(name: str, data: bytes) -> FilesGenerator:
    with zipfile.ZipFile(io.BytesIO(data)) as f:
        for file_info in f.filelist:
            if file_info.is_dir():
                continue

            content = f.read(file_info)
            yield file_info.filename, content


def process_bzip2_file(name: str, data: bytes) -> FilesGenerator:
    yield name.replace(".bz2", ""), bz2.decompress(data)


def process_gzip_file(name: str, data: bytes) -> FilesGenerator:
    yield name.replace(".gz", ""), gzip.decompress(data)


def process_xz_file(name: str, data: bytes) -> FilesGenerator:
    yield name.replace(".xz", ""), lzma.decompress(data)


def is_asset_relevant(variant_name: str) -> bool:
    """
    Estimate the target OS, and kind of asset of check if is relevant for the feed.
    :param variant_name: The name of the asset variant to check.
    :returns: True if the asset is for a relevant OS, False otherwise.
    """
    OS_KEYWORDS = [
        "darwin",
        "macos",
        "mac64",
        "win32",
        "win64",
        "windows",
        "android",
        "freebsd",
        "netbsd",
        "openbsd",
        "dragonfly",
        "plan9",
        "solaris",
    ]
    META_FILES = ["sha256sums", "checksum"]
    variant_name = variant_name.lower()
    return not any(kw in variant_name for kw in OS_KEYWORDS + META_FILES)


def is_text_file(name: str, content: bytes) -> bool:
    is_binary = any(content.startswith(magic.value) for magic in Magic)
    if is_binary:
        return False
    try:
        # only text files can be decoded with UTF-8, binaries will raise an exception
        content.decode("utf-8")
        return True
    except UnicodeDecodeError:
        return False
    except Exception as e:
        logger.error(f"Other exception when testing file type of '{name}':", e)
    return False


def is_binary(content: bytes) -> bool:
    MAGIC_BYTES = [
        Magic.ELF,
    ]
    return any(content.startswith(magic.value) for magic in MAGIC_BYTES)


def is_unsupported_os_binary(name: str, content: bytes) -> bool:
    MAGIC_BYTES = [
        Magic.MACHO_64,
        Magic.MACHO_32,
        Magic.MACHO_MULTI,
        Magic.PE,
    ]
    return any(content.startswith(magic.value) for magic in MAGIC_BYTES)


def is_compressed_file(name: str, content: bytes) -> bool:
    MAGIC_BYTES = [
        b"PK",  # ZIP file
        b"BZ",
    ]
    return any(content.startswith(magic) for magic in MAGIC_BYTES)


def is_shared_library(name: str, *args) -> bool:
    return name.endswith(".so") or name.endswith(".dll") or name.endswith(".sys")


def update_malware_entries(
    repo: Repo,
    malware: Malware,
    last_n_months: int = 6,
    force_latest_release: bool = False,
) -> int:
    """
    Update the entries of a malware in the feed with the latest releases from a repository.
    If the malware has previous versions, new releases are all up to the last known version.
    If no previous version is in the feed, then the repository is queried for releases up to `last_n_months` ago.
    :param repo: The repository to fetch the releases from.
    :param malware: the malware to update.
    :param last_n_months: The number of months to go back in time to fetch releases.
    :param force_latest_release: If True, the latest release is always fetched, regardless of other constraints.
    :return: The number of updates for the malware.
    """

    # the category of the malware is defined in the config file.
    # Repos are loaded from this config file, so they contain the correct name of the category.
    if malware.category != repo.category:
        malware.category = repo.category

    releases = repo.get_releases(
        None,
        last_n_months,
        force_latest_release,
        # malware.get_latest_version(), last_n_months, force_latest_release
    )
    releases = filter_unlisted_releases(malware, releases)

    total_num_assets = _count_all_assets(releases)
    updates = 0

    if total_num_assets == 0:
        logger.info(
            f"No new releases found for '{malware.name}' in the last {last_n_months} months."
        )
        return 0
    else:
        logger.info(
            f"Updating {malware.name}: got {len(releases)} releases ({total_num_assets} assets)"
        )

    for release in releases:
        logger.debug(
            f"processing '{malware.name}' {release.get("tag_name", "(no tag name)")}"
        )
        version, variant_hashes = repo.get_asset_hashes_from_release(release)
        if version not in malware.versions:
            malware.versions.append(version)

        for variant, file_hash in variant_hashes:
            # resolve any duplicates by using latest/longest option
            if (prior_entry := malware.hashes.get(file_hash, None)) is not None:
                prior_version, prior_variant = prior_entry
                # always use latest version (as it's the most recent information)
                version = max(version, prior_version)
                variant = max(variant, prior_variant)  # use most explicit variant
            malware.hashes[file_hash] = version, variant
            updates += 1
    return updates


def filter_unlisted_releases(malware: Malware, releases: list[dict]) -> list[dict]:
    filtered_releases = []
    for release in releases:
        version = normalize_version(release.get("tag_name", ""))
        if version not in malware.versions:
            filtered_releases.append(release)
    logger.debug(
        f"Ignoring {len(releases) - len(filtered_releases)} releases because these are already listed"
    )
    return filtered_releases


def _count_all_assets(repos: list[dict]) -> int:
    total = 0
    for repo in repos:
        total += len(repo.get("assets", []))
    return total


def deduplicate_entries(malwares: list[Malware], repos: list[Repo]) -> list[Malware]:
    """
    Deduplicate the hashes across all malware entries.
    Deduplication is done very naivly by keeping the first entry of the binary (without regarding provided information/source repo)
    It's possible, that malicious assets are mirrored in other repositories (e.g. to avoid detection).
    If more entries have the same hash, it means they are the same binary, so merge the respective information and keep only one version.
    :param malwares: The list of malware entries to deduplicate.
    :param repos: The list of repositories that provide additional information which malware info to keep.
    :return: The deduplicated list of malware entries.
    """
    seen_hashes = set()
    for m in malwares:
        keep_hashes = {}
        drop_hashes = {}
        for h, v in m.hashes.items():
            if h in seen_hashes:
                drop_hashes[h] = v
            else:
                keep_hashes[h] = v
            seen_hashes.add(h)
        m.hashes = keep_hashes
        if len(drop_hashes) > 0:
            logger.info(f"Dropped {len(drop_hashes)} duplicate hashes for '{m.name}'")

    return malwares


def main():
    REPOS_SOURCE_FILE = "./config.ini"
    FEED_DIR = Path("./data")
    FEED_FILE = FEED_DIR / "latest.csv"
    LAST_N_MONTHS = 12

    repos = load_tracked_repos(REPOS_SOURCE_FILE)
    known_entries = load_feed(FEED_FILE)

    # group entries by their name and get lates version
    malwares = group_entries_by_name(known_entries)

    # if there is a Malware in the feed which is no longer tracked, it will be kept in the feed but not updated
    num_updates = 0
    for repo in repos:
        malware = malwares.get(repo.malware_name, None)
        is_new_malware = malware is None
        if is_new_malware:
            logger.debug(f"New Malware '{repo.malware_name}' added to feed")
            malwares[repo.malware_name] = malware = Malware(
                repo.malware_name, repo.category, {}
            )
        num_updates += update_malware_entries(
            repo,
            malware,
            last_n_months=LAST_N_MONTHS,
            force_latest_release=is_new_malware,
        )

    if num_updates > 0:
        malwares = deduplicate_entries(list(malwares.values()), repos)
        # override latest feed and provide a copy of today as backup
        save_feed(FEED_FILE, malwares)
        today = datetime.date.today().strftime("%Y-%m-%d")
        shutil.copy(FEED_FILE, FEED_DIR / f"{today}.csv")
    else:
        logger.info(f"No updates found for {len(repos)} tracked repositories")


if __name__ == "__main__":
    main()
