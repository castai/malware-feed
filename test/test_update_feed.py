import bz2
from dataclasses import dataclass
import datetime
from functools import partial
import gzip
import io
import itertools
import lzma
from pathlib import Path
from queue import Queue
import tarfile
from typing import Callable
from unittest import mock
from unittest.mock import MagicMock
import zipfile

import pytest
import src.update_feed as src


class Spy:
    def __init__(self, func):
        self.func = func
        self.called = 0
        self.args = None
        self.result = None

    def __call__(self, *args, **kwargs):
        self.called += 1
        self.args = args
        self.result = self.func(*args, **kwargs)
        return self.result

    def assert_called_once(self):
        assert self.called == 1


class TestParseGithubReleaseLinkHeader:
    def test_no_link_header_returns_null(self):
        assert src._parse_link_header({}) is None

    def test_only_next_and_last_in_heaer(self):
        base_url = "https://api.github.com/repositories/123/releases"
        next = base_url + "?page=2"
        last = base_url + "?page=7"
        links = f'<{next}>; rel="next", <{last}>; rel="last"'

        links = {
            "first": None,
            "prev": None,
            "next": base_url + "?page=4",
            "last": base_url + "?page=5",
        }

        link_entries = ", ".join(
            f"<{url}>; rel={name}" for name, url in links.items() if url is not None
        )
        pages = src._parse_link_header({"Link": link_entries})
        assert pages is not None
        assert pages == links

    def test_all_entries(self):
        base_url = "https://api.github.com/repositories/123/releases"
        links = {
            "first": base_url + "?page=1",
            "prev": base_url + "?page=2",
            "next": base_url + "?page=4",
            "last": base_url + "?page=5",
        }

        link_entries = ", ".join(f"<{url}>; rel={name}" for name, url in links.items())
        pages = src._parse_link_header({"Link": link_entries})
        assert pages is not None
        assert pages == links


class TestForkRetrieval:
    def test_fork_of_a_fork_has_only_original_name(self):
        original_name = "BAD_STUFF"
        repo = src.GithubRepo(
            "github.com/name/owner",
            "crypto",
            original_name + " (fork/one)",
            is_fork=True,
        )

        name = original_name + "++"
        owner = "hackerman"
        # patch the code that fetches the forsk from GitHub
        repo._fetch_forks = lambda: [
            src.GithubForkNode(
                name=name,
                owner=owner,
                updatedAt=datetime.datetime.now(),
                url="",
                latestRelease=src.GithubReleaseInfo("", "tag", datetime.datetime.now()),
            )
        ]
        forks = list(repo.get_forks())
        assert len(forks) == 1
        new_repo = forks[0]
        assert new_repo.malware_name == f"{original_name} ({owner}/{name})"


class TestVersionParsing:
    def test_regular_semantic_version(self):
        vers = src.normalize_version("1.2.3")
        assert vers == src.SemVer(1, 2, 3)

    def test_major_minor_semantic_version(self):
        vers = src.normalize_version("1.2")
        assert vers == src.SemVer(1, 2, 0)

    def test_only_major_semantic_version(self):
        vers = src.normalize_version("1")
        assert vers == src.SemVer(1, 0, 0)

    def test_semantic_version_with_v_prefix(self):
        vers = src.normalize_version("v1.2.3")
        assert vers == src.SemVer(1, 2, 3)

    def test_only_major_semantic_version_with_v_prefix(self):
        vers = src.normalize_version("v1")
        assert vers == src.SemVer(1, 0, 0)

    def test_semantic_version_with_label(self):
        lbl = "alpha"
        vers = src.normalize_version(f"1.2.3-{lbl}")
        assert vers == src.SemVer(1, 2, 3, label=lbl)

    def test_malformed_label_without_dash(self):
        lbl = "alpha"
        vers = src.normalize_version(f"1.2.3{lbl}")
        assert vers == src.SemVer(1, 2, 3, label=lbl)

    def test_version_with_prefix_and_label(self):
        lbl = "alpha"
        vers = src.normalize_version(f"vers10.20.30{lbl}")
        assert vers == src.SemVer(10, 20, 30, label=lbl)


class TestFilterRelevantAssetsByName:
    @pytest.mark.parametrize(
        "os_indicator",
        [
            "darwin",
            "macos",
            "freebsd",
            "netbsd",
            "openbsd",
            "dragonfly",
            "android",
            "plan9",
            "solaris",
        ],
    )
    def test_filter_irrelevant_os(self, os_indicator):
        asset = f"malware-1.2.3-{os_indicator}-arm64.tar.gz"
        assert not src.is_asset_relevant(asset)

    @pytest.mark.parametrize("indicator", ["SHA256SUMS", "SHA256SUMS.sig", "checksum"])
    def test_filter_release_hash_files(self, indicator):
        assert not src.is_asset_relevant(indicator)

    def test_when_unclear_accept_asset(self):
        asset = "malware-1.2.3.tar.gz"
        assert src.is_asset_relevant(asset)

    @pytest.mark.parametrize("os", ["linux", "ubuntu", "debian", "focal", "bionic"])
    def test_accept_linux(self, os):
        asset = f"malware-1.2.3-{os}.tar.gz"
        assert src.is_asset_relevant(asset)


class TestVersionIsPrintedAsString:
    def test_regular_semantic_version(self):
        lbl = "alpha"
        vers = src.SemVer(1, 2, 3, label=lbl)
        assert str(vers) == f"1.2.3-{lbl}"


class TestAssetProcessorDetermination:
    def test_regular_tar_gz(self):
        name = "archive.tar.gz"
        data = b""
        is_archive, processor_fn = src.determine_asset_processor(name, data)
        assert is_archive
        assert processor_fn == src.process_tarball

    def test_old_tz_archive(self):
        name = "archive.tgz"
        data = b""
        is_archive, processor_fn = src.determine_asset_processor(name, data)
        assert is_archive
        assert processor_fn == src.process_tarball

    def test_tar_bz2(self):
        name = "archive.tar.bz2"
        data = b""
        is_archive, processor_fn = src.determine_asset_processor(name, data)
        assert is_archive
        assert processor_fn == src.process_tarball

    def test_tar_xz(self):
        name = "archive.tar.xz"
        data = b""
        is_archive, processor_fn = src.determine_asset_processor(name, data)
        assert processor_fn == src.process_tarball
        assert is_archive

    def test_metadata_for_tarball_is_ignored(self):
        name = "archive_v1.27.0_darwin_all.tar.gz.cid"
        data = b"just some ASCII text"
        is_archive, processor_fn = src.determine_asset_processor(name, data)
        assert not is_archive
        assert processor_fn is None

    def test_bzip2_file(self):
        name = "archive.bz2"
        data = b""
        is_archive, processor_fn = src.determine_asset_processor(name, data)
        assert not is_archive
        assert processor_fn == src.process_bzip2_file

    def test_gzip_file(self):
        name = "archive.gz"
        data = b""
        is_archive, processor_fn = src.determine_asset_processor(name, data)
        assert not is_archive
        assert processor_fn == src.process_gzip_file

    def test_xz_file(self):
        name = "archive.xz"
        data = b""
        is_archive, processor_fn = src.determine_asset_processor(name, data)
        assert not is_archive
        assert processor_fn == src.process_xz_file

    def test_raw_binary_file(self):
        name = "binary.bin"
        data = b"\x7fELFarstarst"
        is_archive, processor_fn = src.determine_asset_processor(name, data)
        # this is just an anonymous function returning the data itself
        res = processor_fn(name, data)
        assert not is_archive
        assert res == [(name, data)]
        # assert processor_fn == src.process_binary

    def test_python_script(self):
        name = "script.py"
        data = b"print('Hello World!')"
        is_archive, processor_fn = src.determine_asset_processor(name, data)
        # this is just an anonymous function returning the data itself
        res = processor_fn(name, data)
        assert not is_archive
        assert res == [(name, data)]
        # assert processor_fn == src.process_binary

    # def test_7zip(self):
    #     name = "archive.7z"
    #     data = b""
    #     is_archive, processor_fn = src.determine_asset_processor(name, data)
    #     assert is_archive
    #     assert processor_fn == src.process_7zip

    def test_unknown_extension(self):
        name = "archive.xyz"
        data = b""
        is_archive, processor_fn = src.determine_asset_processor(name, data)
        assert not is_archive
        assert processor_fn is None


class TestFileProcessing:
    @pytest.mark.parametrize(
        "compressor, suffix, processor_fn",
        [
            (bz2, "bz2", src.process_bzip2_file),
            (gzip, "gz", src.process_gzip_file),
            (lzma, "xz", src.process_xz_file),
        ],
    )
    def test_compressed_files(self, compressor, suffix, processor_fn):
        expected_name = "my-file"
        file = io.BytesIO()
        data = b"file content"
        with compressor.open(file, "wb") as f:
            f.write(data)
        file.seek(0)  # point to the beginning of the file

        proc_fn = processor_fn(f"{expected_name}.{suffix}", file.read())
        name, content = next(proc_fn)
        assert name == expected_name
        assert content == data


class TestAssetVariantExtraction:
    def test_variant_after_tag(self):
        variant = "darwin-arm64"
        tag = "1.2.3"
        repo_name = "malware"
        release = src.Release(name="", url="", tag=tag)
        asset = src.Asset(f"{repo_name}-{tag}-{variant}.tar.gz", "")
        extracted = src.extract_asset_variant(repo_name, release, asset)
        assert extracted == variant

    def test_variant_between_name_and_tag(self):
        variant = "darwin-arm64"
        tag = "1.2.3"
        repo_name = "malware"
        release = src.Release(name="", url="", tag=tag)
        asset = src.Asset(f"{repo_name}-{variant}-{tag}.tar.gz", "")
        extracted = src.extract_asset_variant(repo_name, release, asset)
        assert extracted == variant

    def test_no_variant(self):
        variant = ""
        tag = "1.2.3"
        repo_name = "malware"
        release = src.Release(name="", url="", tag=tag)
        asset = src.Asset(f"{repo_name}-{tag}.tar.gz", "")
        extracted = src.extract_asset_variant(repo_name, release, asset)
        assert extracted == variant

    def test_variant_in_dot_notation(self):
        variant = "variant"
        tag = "1.2.3"
        repo_name = "malware"
        release = src.Release(name="", url="", tag=tag)
        asset = src.Asset(f"{repo_name}-{tag}.{variant}.tar.gz", "")
        extracted = src.extract_asset_variant(repo_name, release, asset)
        assert extracted == variant

    def test_no_tag_in_asset_name(self):
        variant = "variant"
        tag = ""
        repo_name = "malware"
        release = src.Release(name="", url="", tag=tag)
        asset = src.Asset(f"{repo_name}-{variant}.tar.gz", "")
        extracted = src.extract_asset_variant(repo_name, release, asset)
        assert extracted == variant


class TestDeduplicateFeedEntries:
    def test_no_duplicates(self):
        malwares = []
        res, num_dropped = src.deduplicate_entries(malwares=malwares)
        # no changes were made
        assert malwares == res
        assert num_dropped == 0

    def test_duplicates_keep_first_entry(self):
        malware = src.Malware(
            name="malware-first",
            category="miner",
            hashes={"123": (src.SemVer(0, 0, 0), "")},
        )
        malwares = [
            malware,  # the one with more info is added later
            src.Malware(
                name="malware-second",
                category="hacking",
                hashes={"123": (src.SemVer(1, 2, 3), "darwin-arm64")},
            ),
        ]

        res, num_dropped = src.deduplicate_entries(malwares=malwares)
        assert len(res) == 2  # both entries are still in there
        assert res[0] == malware
        assert len(res[1].hashes) == 0  # hash was removed from 2nd entry
        assert num_dropped == 1  # both entries are still in there


class MockAssetResponse:
    # mock json() method always returns a specific testing dictionary
    @staticmethod
    def read():
        return {"mock_key": "mock_response"}


def _mock_download(name: str, content: bytes, url: str, *args, **kwargs):
    if ".tar" in url:
        ext = url.rsplit(".", maxsplit=1)[-1]
        mode = "" if ext == "tar" else ":" + ext
        fh = io.BytesIO()
        with tarfile.open(fileobj=fh, mode=f"w{mode}") as tar:
            info = tarfile.TarInfo(name)
            info.size = len(content)
            tar.addfile(info, io.BytesIO(initial_bytes=content))
        return fh.getvalue()
    elif url.endswith(".zip"):
        fh = io.BytesIO()
        with zipfile.ZipFile(fh, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
            zf.writestr(name, content)
        return fh.getvalue()
    elif url.endswith(".bz2"):
        return bz2.compress(content)
    elif url.endswith(".gz"):
        return gzip.compress(content)
    elif url.endswith(".xz"):
        return lzma.compress(content)
    else:  # raw file
        return content


class TestAssetProcessing:
    @pytest.mark.parametrize(
        "filetype", [".tar.gz", ".tar", ".zip", ".bz2", ".gz", ".xz", None]
    )
    @pytest.mark.parametrize("variant", ["linux", "focal", "ubuntu", None])
    def test_keep_linux_elf_files(self, monkeypatch, variant, filetype):
        content = src.Magic.ELF.value + b"file content"
        malware = "malware"
        tag = "1.2.3"
        asset_name = "-".join([p for p in [malware, tag, variant] if p is not None])

        # create the mocked download
        monkeypatch.setattr(
            src, "_download_asset", partial(_mock_download, asset_name, content)
        )

        file_name = asset_name
        if filetype is not None:
            file_name += filetype
        file, res_hash, _ = src.get_uncompressed_asset_name_and_hash(
            malware,
            file_name,
            f"https://github.com/user/repo/releases/download/{tag}/{file_name}",
            "linux",
        )
        assert file == asset_name
        assert res_hash is not None

    @pytest.mark.parametrize("filetype", [".tar.gz", ".zip", ".gz", ".xz", None])
    @pytest.mark.parametrize("variant", ["darwin-arm64", "macos", "osx", "mac64", None])
    def test_skip_mac_files(self, monkeypatch, variant, filetype):
        content = src.Magic.MACHO_64.value + b"file content"
        malware = "malware"
        tag = "1.2.3"
        asset_name = "-".join([p for p in [malware, tag, variant] if p is not None])

        # create the mocked download
        monkeypatch.setattr(
            src, "_download_asset", partial(_mock_download, asset_name, content)
        )
        file_name = asset_name
        if filetype is not None:
            file_name += filetype
        _, res_hash, _ = src.get_uncompressed_asset_name_and_hash(
            malware,
            file_name,
            f"https://github.com/user/repo/releases/download/{tag}/{file_name}",
            "linux",
        )
        assert res_hash is None


@pytest.fixture
def setup_repos_source_file(tmp_path):
    """
    Fixture for a factory to generate a config.ini with the requested number of repositories
    """

    def _create_config_ini(num_repos):
        repos_source_file = tmp_path / "config.ini"
        repo_entries = [
            f"repo{i} = https://github.com/user{i}/repo{i}" for i in range(num_repos)
        ]
        content = "\n".join(str(line) for line in ["[test]"] + repo_entries)
        repos_source_file.write_text(content)
        return repos_source_file

    return _create_config_ini


@pytest.fixture
def setup_feed_file(tmp_path):
    """
    Fixture for a factory to generate a feed file with the requested number of entries.
    If a list of hashes is supplied, these will be used for the entries.
    """

    def _create_file(num_entries, hashes: list[str] | None = None):
        feed_dir = tmp_path / "data"
        feed_dir.mkdir(parents=True)

        categories = itertools.cycle(["hacking", "proxy", "crypto"])
        hashes = (f"{i:0<5}" for i in range(num_entries)) if hashes is None else hashes
        entries = [
            src.Malware(
                f"malware{i}",
                next(categories),
                {next(hashes): (src.SemVer(1, 2, 3), "linux")},
            )
            for i in range(num_entries)
        ]
        src.save_feed(feed_dir / "latest.csv", entries)
        return feed_dir, entries

    return _create_file


@pytest.fixture
def setup_stashed_repos():
    """
    Fixture for a factory to generate a list of repos left over from a previous run
    """

    def _create_file(dir: Path, num_entries):
        queue_file = dir / "queue.csv"
        queue_entries = [
            src.GithubRepo("github.com/owner/repo", "hacking", f"malware{i}")
            for i in range(num_entries)
        ]
        src.save_remaining_repos(queue_entries, queue_file)
        return queue_file

    return _create_file


@dataclass
class MainSetup:
    repos_source_file: Path
    feed_dir: Path
    feed_entries: list[src.Malware]
    queue_file: Path
    mock_process: MagicMock | None

    @property
    def main_args(self):
        return self.repos_source_file, self.feed_dir


@pytest.fixture
def setup_main_test(
    monkeypatch,
    setup_repos_source_file,
    setup_feed_file,
    setup_stashed_repos,
):
    def _setup_files(
        num_repos: int = 1,
        num_feed_entries: int = 1,
        num_stashed: int = 0,
        proc_fn: Callable = None,
    ):
        repos_source_file = (
            setup_repos_source_file(num_repos) if num_repos > 0 else None
        )
        feed_dir, feed_entries = (
            setup_feed_file(num_feed_entries)
            if num_feed_entries > 0
            else (repos_source_file.parent, None)
        )
        queue_file = (
            setup_stashed_repos(feed_dir, num_stashed) if num_stashed > 0 else None
        )

        # by default just patch the mock_process function and act like no updates were made
        if proc_fn is None:
            mock_process = MagicMock()
            mock_process.return_value = (0, False)  # 0 updates -> don't write result
            monkeypatch.setattr(src, "process_repository_queue", mock_process)
        else:
            mock_process = None
            monkeypatch.setattr(src, "process_repository_queue", proc_fn)

        # return repos_source_file, feed_dir, feed_entries, queue_file, mock_process
        return MainSetup(
            repos_source_file=repos_source_file,
            feed_dir=feed_dir,
            feed_entries=feed_entries,
            queue_file=queue_file,
            mock_process=mock_process,
        )

    return _setup_files


class TestHighLevelProcessing:
    def test_new_update_with_no_prior_entries(self, setup_main_test, monkeypatch):
        num_repos = 3
        s = setup_main_test(num_repos=num_repos, num_feed_entries=0)

        src.main(*s.main_args)

        s.mock_process.assert_called_once()
        q, feed_entries = s.mock_process.call_args.args
        assert isinstance(q, Queue)
        assert q.qsize() == num_repos  # all the new repos
        assert len(feed_entries) == 0  # no prior entries

    def test_update_with_prior_entries(self, setup_main_test, monkeypatch):
        num_feed_entries = 5
        num_repos = 3
        s = setup_main_test(num_repos=num_repos, num_feed_entries=num_feed_entries)

        src.main(*s.main_args)

        s.mock_process.assert_called_once()
        q, feed_entries = s.mock_process.call_args.args
        assert isinstance(q, Queue)
        assert q.qsize() == num_repos  # the new repos to process
        # the previous feed entries are also supplied to processing
        assert len(feed_entries) == num_feed_entries

    def test_skip_saving_feed_with_no_new_entries(self, setup_main_test, monkeypatch):
        s = setup_main_test(num_repos=3)

        mock_save_feed = MagicMock()
        monkeypatch.setattr(src, "save_feed", mock_save_feed)

        src.main(*s.main_args)
        mock_save_feed.assert_not_called()

    def test_save_latest_feed_only_when_new_entries_found(
        self, setup_main_test, monkeypatch
    ):
        def _add_new_entry(q, feed_entries, **kwargs):
            feed_entries[new_entry.name] = new_entry
            return 1, False

        num_feed_entries = 5
        s = setup_main_test(num_feed_entries=num_feed_entries, proc_fn=_add_new_entry)
        mock_save_feed = MagicMock()
        monkeypatch.setattr(src, "save_feed", mock_save_feed)
        new_entry = src.Malware(
            "new", "hacking", {"123": (src.SemVer(1, 2, 3), "linux")}
        )

        src.main(*s.main_args)

        mock_save_feed.assert_called_once()
        _, res_feed_entries = mock_save_feed.call_args.args
        assert len(res_feed_entries) == num_feed_entries + 1

    def test_entries_are_deduplicated_before_saving(self, setup_main_test, monkeypatch):
        # hashes will be set in add entry fn, so it's a duplicate
        new_entry = src.Malware("new", "hacking", hashes={})

        def _add_new_entry(q, feed_entries, **kwargs):
            duplicate_hash = list(feed_entries["malware0"].hashes.keys())[0]
            new_entry.hashes[duplicate_hash] = (src.SemVer(1, 2, 3), "linux")
            feed_entries[new_entry.name] = new_entry
            return 1, False

        s = setup_main_test(num_repos=3, num_feed_entries=5, proc_fn=_add_new_entry)

        mock_save_feed = MagicMock()
        monkeypatch.setattr(src, "save_feed", mock_save_feed)

        deduplication_spy = Spy(src.deduplicate_entries)
        monkeypatch.setattr(src, "deduplicate_entries", deduplication_spy)

        src.main(*s.main_args)

        deduplication_spy.assert_called_once()
        _, num_dropped = deduplication_spy.result
        assert num_dropped == 1  # the duplicate entry was dropped again
        mock_save_feed.assert_not_called()  # the drop means overall no new entries were added

    def test_update_exceeding_limit_stores_remaining_repos(
        self,
        setup_main_test,
        monkeypatch,
    ):
        new_entry = src.Malware(
            "new", "hacking", {"123": (src.SemVer(1, 2, 3), "linux")}
        )

        def _add_new_entry_with_quota_limit_reached(q, *args, **kwargs):
            q.put(new_entry)  # the new entry is still unprocessed in the queue
            return 1, True  # True = quota limit reached

        mock_save_queue = MagicMock()
        monkeypatch.setattr(src, "save_remaining_repos", mock_save_queue)

        s = setup_main_test(
            proc_fn=_add_new_entry_with_quota_limit_reached,
        )
        src.main(*s.main_args)

        mock_save_queue.assert_called_once()

    def test_continued_update_loads_stashed_repos(self, monkeypatch, setup_main_test):
        mock_load_stashed_repos = MagicMock()
        monkeypatch.setattr(src, "load_stashed_repos", mock_load_stashed_repos)
        s = setup_main_test(num_repos=3, num_feed_entries=5, num_stashed=7)
        src.main(*s.main_args)

        mock_load_stashed_repos.assert_called_once()

    def test_continued_update_skips_initially_tracked_repos(
        self,
        monkeypatch,
        setup_main_test,
    ):
        mock_load_tracked_repos = mock.Mock(wraps=src.load_tracked_repos)
        monkeypatch.setattr(src, "load_tracked_repos", mock_load_tracked_repos)
        s = setup_main_test(num_stashed=1)
        src.main(*s.main_args)
        mock_load_tracked_repos.assert_not_called()

    def test_stashed_repos_file_is_deleted_after_loading(
        self,
        setup_main_test,
    ):
        s = setup_main_test(num_stashed=1)
        assert s.queue_file.exists()
        src.main(*s.main_args)
        assert not s.queue_file.exists()
