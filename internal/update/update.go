package update

import (
	"fmt"
	"log/slog"
	"strings"
	"sync"

	"github.com/castai/malware-feed/internal/config"
	"github.com/castai/malware-feed/internal/feed"
	"github.com/castai/malware-feed/internal/repo"
	"gopkg.in/ini.v1"
)

func LoadConfig(path string) ([]repo.Repo, error) {
	data, err := ini.Load(path)
	repos := []repo.Repo{}

	for _, section := range data.Sections() {
		for _, entry := range section.Keys() {
			// TODO: handle entry without key
			url := entry.String()

			_, path, found := strings.Cut(url, "github.com/")
			if !found {
				slog.Error("Failed to extract GH Repo info from " + url)
			}
			repoInfo := strings.Split(path, "/")
			if len(repoInfo) < 2 {
				slog.Error("Failed to extract GH owner and name from URL " + url)
			}

			// add entry to repos slice
			repos = append(repos, repo.GithubRepo{
				Url:         url,
				Category:    section.Name(),
				MalwareName: entry.Name(),
				Owner:       repoInfo[0],
				RepoName:    repoInfo[1],
			})
		}
	}

	return repos, err
}

func Update() error {
	const FEED_FILE = "./data/latest.csv"
	feedEntries, err := feed.Load(FEED_FILE)
	if err != nil {
		slog.Error("Failed to load Feed")
		return err
	}
	const parallelDownloads = 5
	const lastNMonths = 1

	crawlArgs := config.CrawlArgs{LastNMonths: lastNMonths}

	queue := make(chan repo.Repo, parallelDownloads)
	err = populateQueue(crawlArgs, queue)
	if err != nil {
		slog.Error("Failed to initialize queue")
	}

	numUpdates, err := processRepositoryQueue(queue, crawlArgs)
	if err != nil {
		slog.Error("Failure during processing the repository queue")
	}
	fmt.Printf("Received %d updates\n", numUpdates)

	// TODO: deduplicate entries

	if numUpdates > 0 {
		err = feed.Save(feedEntries, FEED_FILE)
		if err != nil {
			slog.Error("Failed to save feed: ", err)
		}
	}

	// repos := load_tracked_repos(REPOS_SOURCE_FILE)
	// known_entries := load_feed(FEED_FILE)

	// group entries by their name and get lates version
	// malwares := group_entries_by_name(known_entries)

	// if there is a Malware in the feed which is no longer tracked, it will be kept in the feed but not updated
	// num_updates := 0
	// for repo := range repos {
	//     malware = malwares.get(repo.malware_name, None)
	//     is_new_malware = malware is None
	//     if is_new_malware:
	//         slog.debug(f"New Malware '{repo.malware_name}' added to feed")
	//         malwares[repo.malware_name] = malware = Malware(
	//             repo.malware_name, repo.category, {}
	//         )
	//     num_updates += update_malware_entries(
	//         repo,
	//         malware,
	//         last_n_months=LAST_N_MONTHS,
	//         force_latest_release=is_new_malware,
	//     )
	// }

	// if num_updates > 0 {
	//     malwares = deduplicate_entries(list(malwares.values()), repos)
	//     // override latest feed and provide a copy of today as backup
	//     save_feed(FEED_FILE, malwares)
	//     today = datetime.date.today().strftime("%Y-%m-%d")
	//     shutil.copy(FEED_FILE, FEED_DIR / f"{today}.csv")
	// } else {
	//     logger.info(f"No updates found for {len(repos)} tracked repositories")
	// }

	return nil
}

func populateQueue(crawlArgs config.CrawlArgs, queue chan<- repo.Repo) error {
	slog.Info("Updating feed")
	const CONFIG_FILE = "config.ini"
	repos, err := LoadConfig(CONFIG_FILE)
	if err != nil {
		slog.Error("Failed to load config")
		return err
	}

	go func() {
		defer close(queue)

		for i, repo := range repos {
			fmt.Printf("[%d] Add %s to queue\n", i, repo.GetUrl())
			queue <- repo

			go func() {
				if numForks, err := crawlForks(repo, crawlArgs, queue); err != nil {
					slog.Error("Failed to crawl forks")
				} else if numForks > 0 {
					fmt.Printf("Found %d forks for %s\n", numForks, repo.GetUrl())
				}
			}()
		}

	}()

	return nil
}

func crawlForks(repo repo.Repo, crawlArgs config.CrawlArgs, queue chan<- repo.Repo) (int, error) {
	numForks := 0
	// TODO: implement crawling forks
	// forksChannel := make(chan repo.Repo)
	// repo.GetForks(crawlArgs, forksChannel)

	// for fork := range forksChannel {
	// 	numForks++
	// 	queue <- fork
	// }

	return numForks, nil
}

func processRepositoryQueue(queue <-chan repo.Repo, crawlArgs config.CrawlArgs) (int, error) {
	wg := sync.WaitGroup{}
	results := make(chan int)
	for repo := range queue {
		wg.Add(1)
		go func() {
			defer wg.Done()
			numUpdates := checkForUpdates(repo, crawlArgs)
			results <- numUpdates
		}() // TODO: close channel when there is no more update
	}
	go func() {
		wg.Wait()
		close(results)
	}()
	totalNumUpdates := 0
	for numUpdates := range results {
		totalNumUpdates += numUpdates
	}
	return totalNumUpdates, nil
}

func checkForUpdates(repository repo.Repo, crawlArgs config.CrawlArgs) int {
	fmt.Printf("Checking repo: %s\n", repository.GetUrl())
	releaseChannel := make(chan repo.Release)

	numUpdates := 0
	repository.GetReleases(crawlArgs, releaseChannel)
	for release := range releaseChannel {
		fmt.Printf("Release: %s\n", release.Name)
		numUpdates++
	}
	return numUpdates
}
